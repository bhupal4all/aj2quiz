{
  "quiz": [
    {
      "id": 40,
      "name": "AWS Questions",
      "description": "AWS Questionair"
    },
    {
      "questions": [],
      "name": "Cert Leader - Sep 2017",
      "description": "AWS-Solution-Architect-Associate - AWS Certified Solutions Architect - Associate",
      "config": {
        "allowBack": true,
        "allowReview": true,
        "autoMove": false,
        "pageSize": 1,
        "shuffleQuestions": true,
        "shuffleOptions": false,
        "showClock": true,
        "showPager": true
      },
      "pager": {
        "index": 0,
        "size": 1
      },
      "id": 41
    }
  ],
  "questions": [
    {
      "name": "What does Amazon EC2 provide?",
      "options": [
        {
          "name": "Virtual servers in the Cloud",
          "isAnswer": true,
          "questionId": 1,
          "id": 10010
        },
        {
          "name": "A platform to run code (Java, PHP, Python), paying on an hourly basis",
          "isAnswer": false,
          "questionId": 1,
          "id": 10011
        },
        {
          "name": "Computer Clusters in the Cloud",
          "isAnswer": false,
          "questionId": 1,
          "id": 10012
        },
        {
          "name": "Physical servers, remotely managed by the customer",
          "isAnswer": false,
          "questionId": 1,
          "id": 10013
        }
      ],
      "id": 1,
      "questionTypeId": 1,
      "quizId": "40",
      "hint": "Virtual Machines",
      "explanation": "http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html"
    },
    {
      "name": "Amazon SWF is designed to help users____",
      "options": [
        {
          "name": "Design graphical user interface interactions ",
          "isAnswer": false,
          "questionId": 2,
          "id": 10020
        },
        {
          "name": "Manage user identification and authorization ",
          "isAnswer": false,
          "questionId": 2,
          "id": 10021
        },
        {
          "name": " Store Web content",
          "isAnswer": false,
          "questionId": 2,
          "id": 10022
        },
        {
          "name": "Coordinate synchronous and asynchronous tasks which are distributed and fault tolerant ",
          "isAnswer": true,
          "questionId": 2,
          "id": 10023
        }
      ],
      "id": 2,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Can I control if and when MySQL based RDS Instance is upgraded to new supported versions?",
      "options": [
        {
          "name": "No",
          "isAnswer": false,
          "questionId": 3,
          "id": 10030
        },
        {
          "name": "Only in VPC",
          "isAnswer": false,
          "questionId": 3,
          "id": 10031
        },
        {
          "name": "Yes",
          "isAnswer": true,
          "questionId": 3,
          "id": 10032
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 3,
          "id": 10033
        }
      ],
      "id": 3,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "If I modify a DB Instance or the DB parameter group associated with the instance, should I reboot the instance for the changes to take effect? ",
      "options": [
        {
          "name": "No",
          "isAnswer": false,
          "questionId": 4,
          "id": 10040
        },
        {
          "name": "Yes",
          "isAnswer": true,
          "questionId": 4,
          "id": 10041
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 4,
          "id": 10042
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 4,
          "id": 10043
        }
      ],
      "id": 4,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "By default, EBS volumes that are created and attached to an instance at launch are deleted when that instance is terminated. You can modify this behavior by changing the value of the flag____ to false when you launch the instance",
      "options": [
        {
          "name": "DeleteOnTermination ",
          "isAnswer": true,
          "questionId": 5,
          "id": 10050
        },
        {
          "name": "RemoveOnDeletion ",
          "isAnswer": false,
          "questionId": 5,
          "id": 10051
        },
        {
          "name": "RemoveOnTermination ",
          "isAnswer": false,
          "questionId": 5,
          "id": 10052
        },
        {
          "name": "TerminateOnDeletion ",
          "isAnswer": false,
          "questionId": 5,
          "id": 10053
        }
      ],
      "id": 5,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "What are the initial settings of a user created security group? ",
      "options": [
        {
          "name": " Allow all inbound traffic and Allow no outbound traffic ",
          "isAnswer": false,
          "questionId": 6,
          "id": 10060
        },
        {
          "name": " Allow no inbound traffic and Allow no outbound traffic ",
          "isAnswer": false,
          "questionId": 6,
          "id": 10061
        },
        {
          "name": " Allow no inbound traffic and Allow all outbound traffic ",
          "isAnswer": true,
          "questionId": 6,
          "id": 10062
        },
        {
          "name": " Allow all inbound traffic and Allow all outbound traffic ",
          "isAnswer": false,
          "questionId": 6,
          "id": 10063
        }
      ],
      "id": 6,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Will my standby RDS instance be in the same Region as my primary? ",
      "options": [
        {
          "name": "Only for Oracle RDS types ",
          "isAnswer": false,
          "questionId": 7,
          "id": 10070
        },
        {
          "name": "Yes",
          "isAnswer": true,
          "questionId": 7,
          "id": 10071
        },
        {
          "name": "Only if configured at launch",
          "isAnswer": false,
          "questionId": 7,
          "id": 10072
        },
        {
          "name": "No",
          "isAnswer": false,
          "questionId": 7,
          "id": 10073
        }
      ],
      "id": 7,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "What does Amazon Elastic Beanstalk provide? ",
      "options": [
        {
          "name": "A scalable storage appliance on top of Amazon Web Services",
          "isAnswer": false,
          "questionId": 8,
          "id": 10080
        },
        {
          "name": "An application container on top of Amazon Web Services",
          "isAnswer": true,
          "questionId": 8,
          "id": 10081
        },
        {
          "name": "A service by this name doesn't exist",
          "isAnswer": false,
          "questionId": 8,
          "id": 10082
        },
        {
          "name": "A scalable cluster of EC2 instances",
          "isAnswer": false,
          "questionId": 8,
          "id": 10083
        }
      ],
      "id": 8,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "True or False: When using IAM to control access to your RDS resources, the key names that can be used are case sensitive. <br/><br/>For example: <code>aws:CurrentTime is NOT equivalent to AWS:currenttime. </code>",
      "options": [
        {
          "name": "true",
          "isAnswer": true,
          "questionId": 9,
          "id": 10090
        },
        {
          "name": "false",
          "isAnswer": false,
          "questionId": 9,
          "id": 10091
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 9,
          "id": 10092
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 9,
          "id": 10093
        }
      ],
      "id": 9,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "What will be the status of the snapshot until the snapshot is complete? ",
      "options": [
        {
          "name": "running",
          "isAnswer": false,
          "questionId": 10,
          "id": 10100
        },
        {
          "name": "working",
          "isAnswer": false,
          "questionId": 10,
          "id": 10101
        },
        {
          "name": "processing",
          "isAnswer": false,
          "questionId": 10,
          "id": 10102
        },
        {
          "name": "pending",
          "isAnswer": true,
          "questionId": 10,
          "id": 10103
        }
      ],
      "id": 10,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Can we attach an EBS volume to more than one EC2 instance at the same time? ",
      "options": [
        {
          "name": "No",
          "isAnswer": true,
          "questionId": 11,
          "id": 10110
        },
        {
          "name": "Yes",
          "isAnswer": false,
          "questionId": 11,
          "id": 10111
        },
        {
          "name": "Only EC2-optimized EBS volumes ",
          "isAnswer": false,
          "questionId": 11,
          "id": 10112
        },
        {
          "name": "Only in read mode ",
          "isAnswer": false,
          "questionId": 11,
          "id": 10113
        }
      ],
      "id": 11,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "True or False: Automated backups are enabled by default for a new DB Instance",
      "options": [
        {
          "name": "true",
          "isAnswer": true,
          "questionId": 12,
          "id": 10120
        },
        {
          "name": "false",
          "isAnswer": false,
          "questionId": 12,
          "id": 10121
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 12,
          "id": 10122
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 12,
          "id": 10123
        }
      ],
      "id": 12,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "What does the AWS Storage Gateway provide? ",
      "options": [
        {
          "name": "It allows to integrate on-premises IT environments with Cloud Storage",
          "isAnswer": true,
          "questionId": 13,
          "id": 10130
        },
        {
          "name": "A direct encrypted connection to Amazon S3",
          "isAnswer": false,
          "questionId": 13,
          "id": 10131
        },
        {
          "name": "It's a backup solution that provides an on-premises Cloud storage",
          "isAnswer": false,
          "questionId": 13,
          "id": 10132
        },
        {
          "name": "It provides an encrypted SSL endpoint for backups in the Cloud",
          "isAnswer": false,
          "questionId": 13,
          "id": 10133
        }
      ],
      "id": 13,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Amazon RDS automated backups and DB Snapshots are currently supported for only the ____ storage engine",
      "options": [
        {
          "name": "InnoDB",
          "isAnswer": true,
          "questionId": 14,
          "id": 10140
        },
        {
          "name": "MyISAM",
          "isAnswer": false,
          "questionId": 14,
          "id": 10141
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 14,
          "id": 10142
        },
        {
          "name": "",
          "isAnswer": false,
          "questionId": 14,
          "id": 10143
        }
      ],
      "id": 14,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Does DynamoDB support in-place atomic updates?",
      "options": [
        {
          "name": "Yes",
          "isAnswer": true,
          "questionId": 15,
          "id": 10150
        },
        {
          "name": "No",
          "isAnswer": false,
          "questionId": 15,
          "id": 10151
        },
        {
          "name": "It does support in-place non-atomic updates",
          "isAnswer": false,
          "questionId": 15,
          "id": 10152
        },
        {
          "name": "It is not defined",
          "isAnswer": false,
          "questionId": 15,
          "id": 10153
        }
      ],
      "explanation": "DynamoDB supports in-place atomic updates",
      "id": 15,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Your manager has just given you access to multiple VPN connections that someone else has recently set up between all your company's offices. She needs you to make sure that the communication between the VPNs is secure. Which of the following services would be best for providing a low-cost hub-and-spoke model for primary or backup connectMty between these remote offices?<br/>",
      "options": [
        {
          "name": "Amazon Cloud Front",
          "isAnswer": false,
          "questionId": 16,
          "id": 10160
        },
        {
          "name": "AWS Direct Connect",
          "isAnswer": false,
          "questionId": 16,
          "id": 10161
        },
        {
          "name": "AWS CloudHSM",
          "isAnswer": false,
          "questionId": 16,
          "id": 10162
        },
        {
          "name": "AWS Cloud Hub",
          "isAnswer": true,
          "questionId": 16,
          "id": 10163
        }
      ],
      "explanation": "If you have multiple VPN connections, you can provide secure communication between sites using the<br/>AWS VPN CIoudHub. The VPN CIoudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable for customers with multiple branch offices and existing Internet connections who would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectMty between these remote offices.<br/>",
      "id": 16,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Amazon EC2 provides a . It is an HTTP or HTTPS request that uses the HTTP verbs GET or POST",
      "options": [
        {
          "name": "web database",
          "isAnswer": false,
          "questionId": 17,
          "id": 10170
        },
        {
          "name": ".net framework",
          "isAnswer": false,
          "questionId": 17,
          "id": 10171
        },
        {
          "name": "Query API",
          "isAnswer": true,
          "questionId": 17,
          "id": 10172
        },
        {
          "name": "C Library",
          "isAnswer": false,
          "questionId": 17,
          "id": 10173
        }
      ],
      "explanation": "Amazon EC2 provides a Query API. These requests are HTTP or HTTPS requests that use the HTTP verbs GET or POST and a Query parameter named Action",
      "id": 17,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "In Amazon AWS, which of the following statements is true of key pairs?",
      "options": [
        {
          "name": " Key pairs are used only for Amazon SDKs",
          "isAnswer": false,
          "questionId": 18,
          "id": 10180
        },
        {
          "name": "Key pairs are used only for Amazon EC2 and Amazon CIoudFront",
          "isAnswer": true,
          "questionId": 18,
          "id": 10181
        },
        {
          "name": "Key pairs are used only for Elastic Load Balancing and AWS IAM",
          "isAnswer": false,
          "questionId": 18,
          "id": 10182
        },
        {
          "name": "Key pairs are used for all Amazon services",
          "isAnswer": false,
          "questionId": 18,
          "id": 10183
        }
      ],
      "explanation": "Key pairs consist of a public and private key, where you use the private key to create a digital signature, and then AWS uses the corresponding public key to validate the signature. Key pairs are used only for Amazon EC2 and Amazon CIoudFront",
      "id": 18,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Does Amazon DynamoDB support both increment and decrement atomic operations?",
      "options": [
        {
          "name": "Only increment, since decrement are inherently impossible with DynamoDB's data model",
          "isAnswer": false,
          "questionId": 19,
          "id": 10190
        },
        {
          "name": "No, neither increment nor decrement operations",
          "isAnswer": false,
          "questionId": 19,
          "id": 10191
        },
        {
          "name": "Yes, both increment and decrement operations",
          "isAnswer": true,
          "questionId": 19,
          "id": 10192
        },
        {
          "name": "Only decrement, since increment are inherently impossible with DynamoDB's data model",
          "isAnswer": false,
          "questionId": 19,
          "id": 10193
        }
      ],
      "explanation": "Amazon DynamoDB supports increment and decrement atomic operations",
      "questionTypeId": 1,
      "quizId": "40",
      "id": 19
    },
    {
      "name": " An organization has three separate AWS accounts, one each for development, testing, and production. The organization wants the testing team to have access to certain AWS resources in the production account. How can the organization achieve this?",
      "options": [
        {
          "name": "It is not possible to access resources of one account with another account",
          "isAnswer": false,
          "questionId": 20,
          "id": 10200
        },
        {
          "name": "Create the IAM roles with cross account access",
          "isAnswer": true,
          "questionId": 20,
          "id": 10201
        },
        {
          "name": "Create the IAM user in a test account, and allow it access to the production environment with the IAM policy",
          "isAnswer": false,
          "questionId": 20,
          "id": 10202
        },
        {
          "name": "Create the IAM users with cross account access",
          "isAnswer": false,
          "questionId": 20,
          "id": 10203
        }
      ],
      "explanation": "An organization has multiple AWS accounts to isolate a development environment from a testing or production environment. At times the users from one account need to access resources in the other account, such as promoting an update from the development environment to the production environment. In this case the IAM role with cross account access will provide a solution. Cross account access lets one account share access to their resources with users in the other AWS accounts",
      "id": 20,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You need to import several hundred megabytes of data from a local Oracle database to an Amazon RDS DB instance. What does AWS recommend you use to accomplish this?",
      "options": [
        {
          "name": "Oracle export/import utilities",
          "isAnswer": false,
          "questionId": 21,
          "id": 10210
        },
        {
          "name": "Oracle SQL Developer",
          "isAnswer": false,
          "questionId": 21,
          "id": 10211
        },
        {
          "name": "Oracle Data Pump",
          "isAnswer": true,
          "questionId": 21,
          "id": 10212
        },
        {
          "name": "DBMS_FILE_TRANSFER",
          "isAnswer": false,
          "questionId": 21,
          "id": 10213
        }
      ],
      "explanation": "How you import data into an Amazon RDS DB instance depends on the amount of data you have and the number and variety of database objects in your database.<br/>For example, you can use Oracle SQL Developer to import a simple, 20 MB database; you want to use Oracle Data Pump to import complex databases or databases that are several hundred megabytes or several terabytes in size",
      "id": 21,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "A user has created an EBS volume with 1000 IOPS. What is the average IOPS that the user will get for most of the year as per EC2 SLA if the instance is attached to the EBS optimized instance?",
      "options": [
        {
          "name": "950",
          "isAnswer": false,
          "questionId": 22,
          "id": 10220
        },
        {
          "name": "990",
          "isAnswer": false,
          "questionId": 22,
          "id": 10221
        },
        {
          "name": "1000",
          "isAnswer": false,
          "questionId": 22,
          "id": 10222
        },
        {
          "name": "900",
          "isAnswer": true,
          "questionId": 22,
          "id": 10223
        }
      ],
      "explanation": "As per AWS SLA if the instance is attached to an EBS-Optimized instance, then the Provisioned IOPS volumes are designed to deliver within 10% of the provisioned IOPS performance 99.9% of the time in a given year. Thus, if the user has created a volume of 1000 IOPS, the user will get a minimum 900 IOPS 99.9% time of the year",
      "id": 22,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You need to migrate a large amount of data into the cloud that you have stored on a hard disk and you decide that the best way to accomplish this is with AWS Import/Export and you mail the hard disk to AWS. Which of the following statements is incorrect in regards to AWS Import/Export?",
      "options": [
        {
          "name": "It can export from Amazon S3",
          "isAnswer": false,
          "questionId": 23,
          "id": 10230
        },
        {
          "name": "It can Import to Amazon Glacier",
          "isAnswer": false,
          "questionId": 23,
          "id": 10231
        },
        {
          "name": "It can export from Amazon Glacier",
          "isAnswer": true,
          "questionId": 23,
          "id": 10232
        },
        {
          "name": "It can Import to Amazon EBS ",
          "isAnswer": false,
          "questionId": 23,
          "id": 10233
        }
      ],
      "explanation": "AWS Import/Export supports: Import to Amazon S3<br/>Export from Amazon S3 Import to Amazon EBS Import to Amazon Glacier",
      "questionTypeId": 1,
      "quizId": "40",
      "id": 23
    },
    {
      "name": "You are in the process of creating a Route 53 DNS failover to direct traffic to two EC2 zones. Obviously, if one fails, you would like Route 53 to direct traffic to the other region. Each region has an ELB with some instances being distributed. What is the best way for you to configure the Route 53 health check?",
      "options": [
        {
          "name": "Route 53 doesn't support ELB with an internal health check.You need to create your own Route 53 health check of the ELB",
          "isAnswer": false,
          "questionId": 24,
          "id": 10240
        },
        {
          "name": "Route 53 natively supports ELB with an internal health check. Turn \"Eva|uate target health\" off and \"Associate with Health Check\" on and R53 will use the ELB's internal health check",
          "isAnswer": false,
          "questionId": 24,
          "id": 10241
        },
        {
          "name": "Route 53 doesn't support ELB with an internal health check. You need to associate your resource record set for the ELB with your own health check",
          "isAnswer": false,
          "questionId": 24,
          "id": 10242
        },
        {
          "name": "Route 53 natively supports ELB with an internal health check. Turn \"Eva|uate target health\" on and \"Associate with Health Check\" off and R53 will use the ELB's internal health check",
          "isAnswer": true,
          "questionId": 24,
          "id": 10243
        }
      ],
      "explanation": "With DNS Failover, Amazon Route 53 can help detect an outage of your website and redirect your end users to alternate locations where your application is operating properly. When you enable this feature, Route 53 uses health checks-regularly making Internet requests to your appIication’s endpoints from multiple locations around the world-to determine whether each endpoint of your application is up or down.<br/>To enable DNS Failover for an ELB endpoint, create an Alias record pointing to the ELB and set the \"EvaIuate Target HeaIth\" parameter to true. Route 53 creates and manages the health checks for your ELB automatically. You do not need to create your own Route 53 health check of the ELB. You also do not need to associate your resource record set for the ELB with your own health check, because Route 53 automatically associates it with the health checks that Route 53 manages on your behalf. The ELB health check will also inherit the health of your backend instances behind that ELB.",
      "id": 24,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "A user wants to use an EBS-backed Amazon EC2 instance for a temporary job. Based on the input data, the job is most likely to finish within a week. Which of the following steps should be followed to terminate the instance automatically once the job is finished?",
      "options": [
        {
          "name": "Configure the EC2 instance with a stop instance to terminate it",
          "isAnswer": false,
          "questionId": 25,
          "id": 10250
        },
        {
          "name": "Configure the EC2 instance with ELB to terminate the instance when it remains idle",
          "isAnswer": false,
          "questionId": 25,
          "id": 10251
        },
        {
          "name": "Configure the CIoudWatch alarm on the instance that should perform the termination action once the instance is idle",
          "isAnswer": true,
          "questionId": 25,
          "id": 10252
        },
        {
          "name": " Configure the Auto Scaling schedule actMty that terminates the instance after 7 days",
          "isAnswer": false,
          "questionId": 25,
          "id": 10253
        }
      ],
      "explanation": "Auto Scaling can start and stop the instance at a pre-defined time. Here, the total running time is unknown. Thus, the user has to use the CIoudWatch alarm, which monitors the CPU utilization. The user can create an alarm that is triggered when the average CPU utilization percentage has been lower than 10 percent<br/>for 24 hours, signaling that it is idle and no longer in use. When the utilization is below the threshold limit, it will terminate the instance as a part of the instance action.",
      "id": 25,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Which of the following is true of Amazon EC2 security group?",
      "options": [
        {
          "name": "You can modify the outbound rules for EC2-Classic",
          "isAnswer": false,
          "questionId": 26,
          "id": 10260
        },
        {
          "name": "You can modify the rules for a security group only if the security group controls the traffic for just one instance",
          "isAnswer": false,
          "questionId": 26,
          "id": 10261
        },
        {
          "name": "You can modify the rules for a security group only when a new instance is created",
          "isAnswer": false,
          "questionId": 26,
          "id": 10262
        },
        {
          "name": "You can modify the rules for a security group at any time",
          "isAnswer": true,
          "questionId": 26,
          "id": 10263
        }
      ],
      "explanation": "A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you associate one or more security groups with the instance. You add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.",
      "id": 26,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "An Elastic IP address (EIP) is a static IP address designed for dynamic cloud computing. With an EIP, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Your EIP is associated with your AWS account, not a particular EC2 instance, and it  remains associated with your account until you choose to explicitly release it. By default how many EIPs  is each AWS account limited to on a per region basis?",
      "options": [
        {
          "name": "1",
          "isAnswer": false,
          "questionId": 27,
          "id": 10270
        },
        {
          "name": "5",
          "isAnswer": true,
          "questionId": 27,
          "id": 10271
        },
        {
          "name": "Unlimited",
          "isAnswer": false,
          "questionId": 27,
          "id": 10272
        },
        {
          "name": "10",
          "isAnswer": false,
          "questionId": 27,
          "id": 10273
        }
      ],
      "explanation": "By default, all AWS accounts are limited to 5 Elastic IP addresses per region for each AWS account, because public (IPv4) Internet addresses are a scarce public resource. AWS strongly encourages you to use an EIP primarily for load balancing use cases, and use DNS hostnames for all other inter-node communication.<br/>If you feel your architecture warrants additional EIPs, you would need to complete the Amazon EC2 Elastic IP Address Request Form and give reasons as to your need for additional addresses",
      "id": 27,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "In Amazon EC2, partial instance-hours are billed",
      "options": [
        {
          "name": "per second used in the hour",
          "isAnswer": false,
          "questionId": 28,
          "id": 10280
        },
        {
          "name": "per minute used",
          "isAnswer": false,
          "questionId": 28,
          "id": 10281
        },
        {
          "name": "by combining partial segments into full hours",
          "isAnswer": false,
          "questionId": 28,
          "id": 10282
        },
        {
          "name": "as full hours",
          "isAnswer": true,
          "questionId": 28,
          "id": 10283
        }
      ],
      "explanation": "Partial instance-hours are billed to the next hour",
      "id": 28,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "In EC2, what happens to the data in an instance store if an instance reboots (either intentionally or unintentionally)?",
      "options": [
        {
          "name": "Data is deleted from the instance store for security reasons",
          "isAnswer": false,
          "questionId": 29,
          "id": 10290
        },
        {
          "name": "Data persists in the instance store",
          "isAnswer": true,
          "questionId": 29,
          "id": 10291
        },
        {
          "name": "Data is partially present in the instance store",
          "isAnswer": false,
          "questionId": 29,
          "id": 10292
        },
        {
          "name": "Data in the instance store will be lost",
          "isAnswer": false,
          "questionId": 29,
          "id": 10293
        }
      ],
      "explanation": "The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data on instance store volumes is lost under the following circumstances.<br/>Failure of an underlying drive<br/>Stopping an Amazon EBS-backed instance Terminating an instance",
      "id": 29,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You are setting up a VPC and you need to set up a public subnet within that VPC. Which following requirement must be met for this subnet to be considered a public subnet?",
      "options": [
        {
          "name": "Subnet's traffic is not routed to an internet gateway but has its traffic routed to a virtual private gateway",
          "isAnswer": false,
          "questionId": 30,
          "id": 10300
        },
        {
          "name": "Subnet's traffic is routed to an internet gateway",
          "isAnswer": true,
          "questionId": 30,
          "id": 10301
        },
        {
          "name": "Subnet's traffic is not routed to an internet gateway",
          "isAnswer": false,
          "questionId": 30,
          "id": 10302
        },
        {
          "name": "None of these answers can be considered a public subnet",
          "isAnswer": false,
          "questionId": 30,
          "id": 10303
        }
      ],
      "explanation": "A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS cloud. You can launch your AWS resources, such as Amazon EC2 instances, into your VPC. You can configure your VPC: you can select its IP address range, create   subnets, and configure route tables, network gateways, and security settings.<br/>A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a subnet that you select. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won't be connected to the Internet.<br/>If a subnet's traffic is routed to an internet gateway, the subnet is known as a public subnet.<br/>If a subnet doesn't have a route to the internet gateway, the subnet is known as a private subnet.<br/>If a subnet doesn't have a route to the internet gateway, but has its traffic routed to a virtual private gateway, the subnet is known as a VPN-only subnet.<br/>",
      "id": 30,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Can you specify the security group that you created for a VPC when you launch an instance in EC2-Classic?",
      "options": [
        {
          "name": "No, you can specify the security group created for EC2-Classic when you launch a VPC instance",
          "isAnswer": false,
          "questionId": 31,
          "id": 10310
        },
        {
          "name": "No",
          "isAnswer": true,
          "questionId": 31,
          "id": 10311
        },
        {
          "name": "Yes",
          "isAnswer": false,
          "questionId": 31,
          "id": 10312
        },
        {
          "name": "No, you can specify the security group created for EC2-Classic to a non-VPC based instance only",
          "isAnswer": false,
          "questionId": 31,
          "id": 10313
        }
      ],
      "explanation": "If you're using EC2-Classic, you must use security groups created specifically for EC2-Classic. When you launch an instance in EC2-Classic, you must specify a security group in the same region as the instance. You can't specify a security group that you created for a VPC when you launch an instance in EC2-Classic.",
      "id": 31,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "While using the EC2 GET requests as URLs, the is the URL that serves as the entry point for the web service.",
      "options": [
        {
          "name": "token",
          "isAnswer": false,
          "questionId": 32,
          "id": 10320
        },
        {
          "name": "endpoint",
          "isAnswer": true,
          "questionId": 32,
          "id": 10321
        },
        {
          "name": "action",
          "isAnswer": false,
          "questionId": 32,
          "id": 10322
        },
        {
          "name": "None of these",
          "isAnswer": false,
          "questionId": 32,
          "id": 10323
        }
      ],
      "explanation": "The endpoint is the URL that serves as the entry point for the web service",
      "id": 32,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You have been asked to build a database warehouse using Amazon Redshift. You know a little about it, including that it is a SQL data warehouse solution, and uses industry standard ODBC and JDBC connections and PostgreSQL drivers. However you are not sure about what sort of storage it uses for database tables. What sort of storage does Amazon Redshift use for database tables?<br/>",
      "options": [
        {
          "name": "InnoDB Tables",
          "isAnswer": false,
          "questionId": 33,
          "id": 10330
        },
        {
          "name": "NDB data storage",
          "isAnswer": false,
          "questionId": 33,
          "id": 10331
        },
        {
          "name": "Columnar data storage",
          "isAnswer": true,
          "questionId": 33,
          "id": 10332
        },
        {
          "name": "NDB CLUSTER Storage",
          "isAnswer": false,
          "questionId": 33,
          "id": 10333
        }
      ],
      "explanation": "Amazon Redshift achieves efficient storage and optimum query performance through a combination of massively parallel processing, columnar data storage, and very efficient, targeted data compression encoding schemes.<br/>Columnar storage for database tables is an important factor in optimizing analytic query performance because it drastically reduces the overall disk I/O requirements and reduces the amount of data you need to load from disk.<br/>",
      "id": 33,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You are checking the workload on some of your General Purpose (SSD) and Provisioned IOPS (SSD) volumes and it seems that the I/O latency is higher than you require. You should probably check the  to make sure that your application is not trying to drive more IOPS than you have provisioned.",
      "options": [
        {
          "name": "Amount of IOPS that are available",
          "isAnswer": false,
          "questionId": 34,
          "id": 10340
        },
        {
          "name": "Acknowledgement from the storage subsystem",
          "isAnswer": false,
          "questionId": 34,
          "id": 10341
        },
        {
          "name": "Average queue length",
          "isAnswer": true,
          "questionId": 34,
          "id": 10342
        },
        {
          "name": "Time it takes for the I/O operation to complete",
          "isAnswer": false,
          "questionId": 34,
          "id": 10343
        }
      ],
      "explanation": "In EBS workload demand plays an important role in getting the most out of your General Purpose (SSD) and Provisioned IOPS (SSD) volumes. In order for your volumes to deliver the amount of IOPS that are available, they need to have enough I/O requests sent to them. There is a relationship between the demand on the volumes, the amount of IOPS that are available to them, and the latency of the request (the amount of time it takes for the I/O operation to complete).<br/>Latency is the true end-to-end client time of an I/O operation; in other words, when the client sends a IO, how long does it take to get an acknowledgement from the storage subsystem that the IO read or write is complete.<br/>If your I/O latency is higher than you require, check your average queue length to make sure that your application is not trying to drive more IOPS than you have provisioned. You can maintain high IOPS while keeping latency down by maintaining a low average queue length (which is achieved by provisioning   more IOPS for your volume).<br/>",
      "id": 34,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Which of the below mentioned options is not available when an instance is launched by Auto Scaling with EC2 Classic?<br/>",
      "options": [
        {
          "name": "Public IP",
          "isAnswer": false,
          "questionId": 35,
          "id": 10350
        },
        {
          "name": "Elastic IP",
          "isAnswer": true,
          "questionId": 35,
          "id": 10351
        },
        {
          "name": "Private DNS",
          "isAnswer": false,
          "questionId": 35,
          "id": 10352
        },
        {
          "name": "Private IP",
          "isAnswer": false,
          "questionId": 35,
          "id": 10353
        }
      ],
      "explanation": "Auto Scaling supports both EC2 classic and EC2-VPC. When an instance is launched as a part of EC2 classic, it will have the public IP and DNS as well as the private IP and DNS.",
      "id": 35,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You have been given a scope to deploy some AWS infrastructure for a large organisation. The requirements are that you will have a lot of EC2 instances but may need to add more when the average utilization of your Amazon EC2 fileet is high and conversely remove them when CPU utilization is low. Which AWS services would be best to use to accomplish this?",
      "options": [
        {
          "name": "Auto Scaling, Amazon CIoudWatch and AWS Elastic Beanstalk",
          "isAnswer": false,
          "questionId": 36,
          "id": 10360
        },
        {
          "name": "Auto Scaling, Amazon CIoudWatch and Elastic Load Balancing",
          "isAnswer": true,
          "questionId": 36,
          "id": 10361
        },
        {
          "name": "Amazon CIoudFront, Amazon CIoudWatch and Elastic Load Balancing",
          "isAnswer": false,
          "questionId": 36,
          "id": 10362
        },
        {
          "name": "AWS Elastic Beanstalk , Amazon CIoudWatch and Elastic Load Balancing",
          "isAnswer": false,
          "questionId": 36,
          "id": 10363
        }
      ],
      "explanation": "Auto Scaling enables you to follow the demand curve for your applications closely, reducing the need to manually provision Amazon EC2 capacity in advance. For example, you can set a condition to add new<br/>Amazon EC2 instances in increments to the Auto Scaling group when the average utilization of your Amazon EC2 fileet is high; and similarly, you can set a condition to remove instances in the same increments when CPU utilization is low. If you have predictable load changes, you can set a schedule through Auto Scaling to plan your scaling actMties. You can use Amazon CIoudWatch to send alarms to trigger scaling actMties and Elastic Load Balancing to help distribute traffic to your instances within Auto Scaling groups. Auto Scaling enables you to run your Amazon EC2 fileet at optimal utilization.<br/>",
      "id": 36,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You are building infrastructure for a data warehousing solution and an extra request has come through that there will be a lot of business reporting queries running all the time and you are not sure if your current DB instance will be able to handle it. What would be the best solution for this?",
      "options": [
        {
          "name": "DB Parameter Groups",
          "isAnswer": false,
          "questionId": 37,
          "id": 10370
        },
        {
          "name": "Read Replicas",
          "isAnswer": true,
          "questionId": 37,
          "id": 10371
        },
        {
          "name": "Multi-AZ DB Instance deployment",
          "isAnswer": false,
          "questionId": 37,
          "id": 10372
        },
        {
          "name": "Database Snapshots",
          "isAnswer": false,
          "questionId": 37,
          "id": 10373
        }
      ],
      "explanation": "Read Replicas make it easy to take advantage of MySQL’s built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. There are a variety of scenarios where deploying one or more Read Replicas for a given source DB Instance may make sense. Common reasons for deploying a Read Replica include:<br/>Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas.<br/>Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read RepIica(s). For this use case, keep in mind that the data on the Read Replica may be \"staIe\" since the source DB Instance is unavailable.<br/>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance.",
      "id": 37,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "In DynamoDB, could you use IAM to grant access to Amazon DynamoDB resources and API actions?",
      "options": [
        {
          "name": "In DynamoDB there is no need to grant access",
          "isAnswer": false,
          "questionId": 38,
          "id": 10380
        },
        {
          "name": "Depended to the type of access",
          "isAnswer": false,
          "questionId": 38,
          "id": 10381
        },
        {
          "name": "No",
          "isAnswer": false,
          "questionId": 38,
          "id": 10382
        },
        {
          "name": "Yes",
          "isAnswer": true,
          "questionId": 38,
          "id": 10383
        }
      ],
      "explanation": "Amazon DynamoDB integrates with AWS Identity and Access Management (IAM). You can use AWS IAM to grant access to Amazon DynamoDB resources and API actions. To do this, you first write an AWS IAM policy, which is a document that explicitly lists the permissions you want to grant. You then attach that  policy to an AWS IAM user or role",
      "id": 38,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Much of your company's data does not need to be accessed often, and can take several hours for retrieval time, so it's stored on Amazon Glacier. However someone within your organization has expressed concerns that his data is more sensitive than the other data, and is wondering whether the high<br/>level of encryption that he knows is on S3 is also used on the much cheaper Glacier service. Which of the following statements would be most applicable in regards to this concern?",
      "options": [
        {
          "name": "There is no encryption on Amazon Glacier, that's why it is cheaper",
          "isAnswer": false,
          "questionId": 39,
          "id": 10390
        },
        {
          "name": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than Amazon S3 but you can change it to AES-256 if you are willing to pay more",
          "isAnswer": false,
          "questionId": 39,
          "id": 10391
        },
        {
          "name": "Amazon Glacier automatically encrypts the data using AES-256, the same as Amazon S3",
          "isAnswer": true,
          "questionId": 39,
          "id": 10392
        },
        {
          "name": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than Amazon S3.",
          "isAnswer": false,
          "questionId": 39,
          "id": 10393
        }
      ],
      "explanation": "Like Amazon S3, the Amazon Glacier service provides low-cost, secure, and durable storage. But where S3 is designed for rapid retrieval, Glacier is meant to be used as an archival service for data that is not accessed often, and for which retrieval times of several hours are suitable.<br/>Amazon Glacier automatically encrypts the data using AES-256 and stores it durably in an immutable form. Amazon Glacier is designed to provide average annual durability of 99.999999999% for an archive. It stores each archive in multiple facilities and multiple devices. Unlike traditional systems which can require laborious data verification and manual repair, Glacier performs regular, systematic data integrity checks, and is built to be automatically self-healing.",
      "id": 39,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Your EBS volumes do not seem to be performing as expected and your team leader has requested you look into improving their performance. Which of the following is not a true statement relating to the performance of your EBS volumes?",
      "options": [
        {
          "name": "Frequent snapshots provide a higher level of data durability and they will not degrade the performance of your application while the snapshot is in progress",
          "isAnswer": true,
          "questionId": 40,
          "id": 10400
        },
        {
          "name": "General Purpose (SSD) and Provisioned IOPS (SSD) volumes have a throughput limit of 128 MB/s per volume",
          "isAnswer": false,
          "questionId": 40,
          "id": 10401
        },
        {
          "name": "There is a relationship between the maximum performance of your EBS volumes, the amount of I/O you are drMng to them, and the amount of time it takes for each transaction to complete",
          "isAnswer": false,
          "questionId": 40,
          "id": 10402
        },
        {
          "name": "There is a 5 to 50 percent reduction in IOPS when you first access each block of data on a newly created or restored EBS volume",
          "isAnswer": false,
          "questionId": 40,
          "id": 10403
        }
      ],
      "explanation": "Several factors can affect the performance of Amazon EBS volumes, such as instance configuration, I/O characteristics, workload demand, and storage configuration.<br/>Frequent snapshots provide a higher level of data durability, but they may slightly degrade the<br/>performance of your application while the snapshot is in progress. This trade off becomes critical when you have data that changes rapidly. Whenever possible, plan for snapshots to occur during off-peak times in order to minimize workload impact.",
      "id": 40,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You've created your first load balancer and have registered your EC2 instances with the load balancer. Elastic Load Balancing routinely performs health checks on all the registered EC2 instances and automatically distributes all incoming requests to the DNS name of your load balancer across your registered, healthy EC2 instances. By default, the load balancer uses the _ protocol for checking the health of your instances.",
      "options": [
        {
          "name": "HTTPS",
          "isAnswer": false,
          "questionId": 41,
          "id": 10410
        },
        {
          "name": "HTTP",
          "isAnswer": true,
          "questionId": 41,
          "id": 10411
        },
        {
          "name": "ICMP",
          "isAnswer": false,
          "questionId": 41,
          "id": 10412
        },
        {
          "name": "IPv6",
          "isAnswer": false,
          "questionId": 41,
          "id": 10413
        }
      ],
      "explanation": "In Elastic Load Balancing a health configuration uses information such as protocol, ping port, ping path (URL), response timeout period, and health check interval to determine the health state of the instances registered with the load balancer.<br/>Currently, HTTP on port 80 is the default health check. ",
      "questionTypeId": 1,
      "quizId": "40",
      "id": 41
    },
    {
      "name": "A major finance organisation has engaged your company to set up a large data mining application. Using AWS you decide the best service for this is Amazon Elastic MapReduce(EMR) which you know uses Hadoop. Which of the following statements best describes Hadoop?<br/>",
      "options": [
        {
          "name": "Hadoop is 3rd Party software which can be installed using AMI",
          "isAnswer": false,
          "questionId": 42,
          "id": 10420
        },
        {
          "name": "Hadoop is an open source python web framework",
          "isAnswer": false,
          "questionId": 42,
          "id": 10421
        },
        {
          "name": "Hadoop is an open source Java software framework",
          "isAnswer": true,
          "questionId": 42,
          "id": 10422
        },
        {
          "name": "Hadoop is an open source javascript framework ",
          "isAnswer": false,
          "questionId": 42,
          "id": 10423
        }
      ],
      "explanation": "Amazon EMR uses Apache Hadoop as its distributed data processing engine.<br/>Hadoop is an open source, Java software framework that supports data-intensive distributed applications running on large clusters of commodity hardware. Hadoop implements a programming model named \"MapReduce,\" where the data is dMded into many small fragments of work, each of which may be executed on any node in the cluster.<br/>This framework has been widely used by developers, enterprises and startups and has proven to be a reliable software platform for processing up to petabytes of data on clusters of thousands of commodity machines.<br/>",
      "id": 42,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "In Amazon EC2 Container Service, are other container types supported?<br/>",
      "options": [
        {
          "name": "Yes, EC2 Container Service supports any container service you need",
          "isAnswer": false,
          "questionId": 43,
          "id": 10430
        },
        {
          "name": "Yes, EC2 Container Service also supports Microsoft container service",
          "isAnswer": false,
          "questionId": 43,
          "id": 10431
        },
        {
          "name": "No, Docker is the only container platform supported by EC2 Container Service presently",
          "isAnswer": true,
          "questionId": 43,
          "id": 10432
        },
        {
          "name": "Yes, EC2 Container Service supports Microsoft container service and Openstack",
          "isAnswer": false,
          "questionId": 43,
          "id": 10433
        }
      ],
      "explanation": "In Amazon EC2 Container Service, Docker is the only container platform supported by EC2 Container Service presently",
      "id": 43,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "is a fast, filexible, fully managed push messaging service",
      "options": [
        {
          "name": "Amazon SNS",
          "isAnswer": true,
          "questionId": 44,
          "id": 10440
        },
        {
          "name": "Amazon SES",
          "isAnswer": false,
          "questionId": 44,
          "id": 10441
        },
        {
          "name": "Amazon SQS",
          "isAnswer": false,
          "questionId": 44,
          "id": 10442
        },
        {
          "name": "Amazon FPS",
          "isAnswer": false,
          "questionId": 44,
          "id": 10443
        }
      ],
      "explanation": "Amazon Simple Notification Service (Amazon SNS) is a fast, filexible, fully managed push messaging service. Amazon SNS makes it simple and cost-effective to push to mobile devices such as iPhone, iPad, Android, Kindle Fire, and internet connected smart devices, as well as pushing to other distributed service",
      "id": 44,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "1. You are designing an intrusion detection prevention (IDS/IPS) solution for a customer web application in a  single VPC. You are considering the options for implementing IOS IPS protection for traffic coming from theInternet.Which of the following options would you consider? (Choose 2 answers)",
      "options": [
        {
          "name": "Implement IDS/IPS agents on each Instance running In VPC  ",
          "isAnswer": false,
          "questionId": 45,
          "id": 10450
        },
        {
          "name": "Configure an instance in each subnet to switch its network interface card to promiscuous mode and  analyze network traffic.  ",
          "isAnswer": false,
          "questionId": 45,
          "id": 10451
        },
        {
          "name": "Implement Elastic Load Balancing with SSL listeners In front of the web applications  ",
          "isAnswer": true,
          "questionId": 45,
          "id": 10452
        },
        {
          "name": "Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server.",
          "isAnswer": true,
          "questionId": 45,
          "id": 10453
        }
      ],
      "questionTypeId": 1,
      "quizId": "40",
      "id": 45
    },
    {
      "name": "Your customer is willing to consolidate their log streams (access logs application logs security logs etc.) in  one single system. Once consolidated, the customer wants to analyze these logs in real time based on  heuristics. From time to time, the customer needs to validate heuristics, which requires going back to data  samples extracted from the last 12 hours?  What is the best approach to meet your customer’s requirements?  ",
      "options": [
        {
          "name": "Send all the log events to Amazon SQS. Setup an Auto Scaling group of EC2 servers to consume the logs  and apply the heuristics.  ",
          "isAnswer": false,
          "questionId": 46,
          "id": 10460
        },
        {
          "name": "Send all the log events to Amazon Kinesis develop a client process to apply heuristics on the logs  ",
          "isAnswer": false,
          "questionId": 46,
          "id": 10461
        },
        {
          "name": "Configure Amazon Cloud Trail to receive custom logs, use EMR to apply heuristics the logs  ",
          "isAnswer": true,
          "questionId": 46,
          "id": 10462
        },
        {
          "name": "Setup an Auto Scaling group of EC2 syslogd servers, store the logs on S3 use EMR to apply heuristics on  the logs",
          "isAnswer": false,
          "questionId": 46,
          "id": 10463
        }
      ],
      "id": 46,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You require the ability to analyze a customer\\'s clickstream data on a website so they can do behavioral  analysis. Your customer needs to know what sequence of pages and ads their customer clicked on. This data  will be used in real time to modify the page layouts as customers click through the site to increase stickiness  and advertising click-through. Which option meets the requirements for captioning and analyzing this data?  ",
      "options": [
        {
          "name": "Log clicks in weblogs by URL store to Amazon S3, and then analyze with Elastic MapReduce  ",
          "isAnswer": false,
          "questionId": 47,
          "id": 10470
        },
        {
          "name": "Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers  ",
          "isAnswer": true,
          "questionId": 47,
          "id": 10471
        },
        {
          "name": "Write click events directly to Amazon Redshift and then analyze with SQL ",
          "isAnswer": false,
          "questionId": 47,
          "id": 10472
        },
        {
          "name": "Publish web clicks by session to an Amazon SQS queue men periodically drain these events to Amazon  RDS and analyze with sol  ",
          "isAnswer": false,
          "questionId": 47,
          "id": 10473
        }
      ],
      "id": 47,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You are designing a connectivity solution between on-premises infrastructure and Amazon VPC Your  server’s on-premises will De communicating with your VPC instances You will De establishing IPSec tunnels  over the internet You will be using VPN gateways and terminating the IPsec tunnels on AWS-supported  customer gateways.  Which of the following objectives would you achieve by implementing an IPSec tunnel as outlined above?  (Choose 4 answers)  ",
      "options": [
        {
          "name": "End-to-end protection of data in transit  ",
          "isAnswer": false,
          "questionId": 48,
          "id": 10480
        },
        {
          "name": "End-to-end Identity authentication  ",
          "isAnswer": false,
          "questionId": 48,
          "id": 10481
        },
        {
          "name": "Data encryption across the Internet  ",
          "isAnswer": true,
          "questionId": 48,
          "id": 10482
        },
        {
          "name": "Protection of data in transit over the Internet  ",
          "isAnswer": true,
          "questionId": 48,
          "id": 10483
        },
        {
          "name": "Peer identity authentication between VPN gateway and customer gateway  ",
          "isAnswer": true,
          "questionId": 48,
          "id": 10484
        },
        {
          "name": "Data integrity protection across the Internet",
          "isAnswer": true,
          "questionId": 48,
          "id": 10485
        }
      ],
      "id": 48,
      "questionTypeId": 2,
      "quizId": "40"
    },
    {
      "name": "Your company produces customer commissioned one-of-a-kind skiing helmets combining nigh fashion with  custom technical enhancements Customers can show oft their Individuality on the ski slopes and have access  to head-up-displays. GPS rear-view cams and any other technical innovation they wish to embed in the  helmet.  The current manufacturing process is data rich and complex including assessments to ensure that the custom  electronics and materials used to assemble the helmets are to the highest standards Assessments are a  mixture of human and automated assessments you need to add a new set of assessment to model the failure  modes of the custom electronics using GPUs with CUDA. across a cluster of servers with low latency  networking.  What architecture would allow you to automate the existing process using a hybrid approach and ensure that  the architecture can support the evolution of processes over time?",
      "options": [
        {
          "name": "Use AWS Data Pipeline to manage movement of data & meta-data and assessments Use an auto-scaling  group of G2 instances in a placement group.  ",
          "isAnswer": true,
          "questionId": 49,
          "id": 10490
        },
        {
          "name": "Use Amazon Simple Workflow (SWF) 10 manages assessments, movement of data & meta-data Use an  auto-scaling group of G2 instances in a placement group.  ",
          "isAnswer": false,
          "questionId": 49,
          "id": 10491
        },
        {
          "name": "Use Amazon Simple Workflow (SWF) lo manages assessments movement of data & meta-data Use an auto-scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization).  ",
          "isAnswer": false,
          "questionId": 49,
          "id": 10492
        },
        {
          "name": "Use AWS data Pipeline to manage movement of data & meta-data and assessments use auto-scaling  group of C3 with SR-IOV (Single Root I/O virtualization).",
          "isAnswer": false,
          "questionId": 49,
          "id": 10493
        }
      ],
      "id": 49,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "A web-startup runs its very successful social news application on Amazon EC2 with an Elastic Load  Balancer, an Auto-Scaling group of Java/Tomcat application-servers, and DynamoDB as data store. The main  web-application best runs on m2 x large instances since it is highly memory- bound Each new deployment  requires semi-automated creation and testing of a new AMI for the application servers which takes quite a  while ana is therefore only done once per week.  Recently, a new chat feature has been implemented in nodejs and wails to be integrated in the architecture.  First tests show that the new component is CPU bound Because the company has some experience with  using Chef, they decided to streamline the deployment process and use AWS Ops Works as an application  life cycle tool to simplify management of the application and reduce the deployment cycles.  What configuration in AWS Ops Works is necessary to integrate the new chat module in the most costefficient  and flexible way?  ",
      "options": [
        {
          "name": "Create one AWS Ops Works stack, create one AWS Ops Works layer, create one custom recipe  ",
          "isAnswer": false,
          "questionId": 50,
          "id": 10500
        },
        {
          "name": "Create one AWS Ops Works stack create two AWS Ops Works layers create one custom recipe  ",
          "isAnswer": false,
          "questionId": 50,
          "id": 10501
        },
        {
          "name": "Create two AWS Ops Works stacks create two AWS Ops Works layers create one custom recipe  ",
          "isAnswer": true,
          "questionId": 50,
          "id": 10502
        },
        {
          "name": "Create two AWS Ops Works stacks create two AWS Ops Works layers create two custom recipe",
          "isAnswer": false,
          "questionId": 50,
          "id": 10503
        }
      ],
      "id": 50,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "7. You\\'ve been hired to enhance the overall security posture for a very large e-commerce site They have a  well architected multi-tier application running in a VPC that uses ELBs in front of both the web and the app tier  with static assets served directly from S3 They are using a combination of RDS and DynamoOB for their  dynamic data and then archiving nightly into S3 for further processing with EMR They are concerned because  they found questionable log entries and suspect someone is attempting to gain unauthorized access.  Which approach provides a cost effective scalable mitigation to this kind of attack?  ",
      "options": [
        {
          "name": "Recommend mat they lease space at a DirectConnect partner location and establish a 1G DirectConnect  connection to tneirvPC they would then establish Internet connectivity into their space, filter the traffic in  hardware Web Application Firewall (WAF). And then pass the traffic through the DirectConnect connection  into their application running in their VPC.  ",
          "isAnswer": false,
          "questionId": 51,
          "id": 10510
        },
        {
          "name": "Add previously identified hostile source IPs as an explicit INBOUND DENY NACL to the web tier subnet.  ",
          "isAnswer": false,
          "questionId": 51,
          "id": 10511
        },
        {
          "name": "Add a WAF tier by creating a new ELB and an AutoScalmg group of EC2 Instances running a host-based  WAF They would redirect Route 53 to resolve to the new WAF tier ELB The WAF tier would thier pass the traffic to the current web tier The web tier Security  Groups would be updated to only allow traffic from the WAF tier Security Group  ",
          "isAnswer": true,
          "questionId": 51,
          "id": 10512
        },
        {
          "name": "Remove all but TLS 1 2 from the web tier ELB and enable Advanced Protocol Filtering This will enable the  ELB itself to perform WAF functionality",
          "isAnswer": false,
          "questionId": 51,
          "id": 10513
        }
      ],
      "id": 51,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You have deployed a three-tier web application in a VPC with a CIOR block of 10 0 0 0/28 You initially  deploy two web servers, two application servers, two database servers and one NAT instance tor a total of  seven EC2 instances The web. Application and database servers are deployed across two availability zones  (AZs). You also deploy an ELB in front of the two web servers, and use Route53 for DNS Web (raffle  gradually increases in the first few days following the deployment, so you attempt to double the number of  instances in each tier of the application to handle the new load unfortunately some of these new instances fail  to launch.  Which of the following could De the root caused? (Choose 2 answers)  ",
      "options": [
        {
          "name": "The Internet Gateway (IGW) of your VPC has scaled-up adding more instances to handle the traffic spike,  reducing the number of available private IP addresses for new instance launches.  ",
          "isAnswer": false,
          "questionId": 52,
          "id": 10520
        },
        {
          "name": "AWS reserves one IP address In each subnet\\'s CIDR block for Route53 so you do not have enough  addresses left to launch all of the new EC2 instances.  ",
          "isAnswer": false,
          "questionId": 52,
          "id": 10521
        },
        {
          "name": "AWS reserves the first and the last private IP address in each subnet\\'s CIDR block so you do not have  enough addresses left to launch all of the new EC2 instances.  ",
          "isAnswer": true,
          "questionId": 52,
          "id": 10522
        },
        {
          "name": "The ELB has scaled-up. Adding more instances to handle the traffic reducing the number of available  private IP addresses for new instance launches.  ",
          "isAnswer": true,
          "questionId": 52,
          "id": 10523
        },
        {
          "name": "AWS reserves the first tour and the last IP address in each subnet\\'s CIDR block so you do not have  enough addresses left to launch all of the new EC2 instances.",
          "isAnswer": false,
          "questionId": 52,
          "id": 10524
        }
      ],
      "id": 52,
      "questionTypeId": 2,
      "quizId": "40"
    },
    {
      "name": "Refer to the architecture diagram above of a batch processing solution using Simple Queue Service (SOS)  to set up a message queue between EC2 instances which are used as batch processors Cloud Watch  monitors the number of Job requests (queued messages) and an Auto Scaling group adds or deletes batch  servers automatically based on parameters set in Cloud Watch alarms. You can use this architecture to  implement which of the following features in a cost effective and efficient manner?  ",
      "options": [
        {
          "name": "Reduce the overall lime for executing jobs through parallel processing by allowing a busy EC2 instance  that receives a message to pass it to the next instance in a daisy-chain setup.  ",
          "isAnswer": false,
          "questionId": 53,
          "id": 10530
        },
        {
          "name": "Implement fault tolerance against EC2 instance failure since messages would remain in SQS and worn can  continue with recovery of EC2 instances implement fault tolerance against SQS failure by backing up  messages to S3. ",
          "isAnswer": true,
          "questionId": 53,
          "id": 10531
        },
        {
          "name": "Implement message passing between EC2 instances within a batch by exchanging messages through  SOS.  ",
          "isAnswer": false,
          "questionId": 53,
          "id": 10532
        },
        {
          "name": "Coordinate number of EC2 instances with number of job requests automatically thus Improving cost  effectiveness.  ",
          "isAnswer": false,
          "questionId": 53,
          "id": 10533
        },
        {
          "name": "Handle high priority jobs before lower priority jobs by assigning a priority metadata field to SQS messages.",
          "isAnswer": false,
          "questionId": 53,
          "id": 10534
        }
      ],
      "id": 53,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You are running a news website in the eu-west-1 region that updates every 15 minutes. The website has  a world-wide audience it uses an Auto Scaling group behind an Elastic Load Balancer and an Amazon RDS  database Static content resides on Amazon S3, and is distributed through Amazon CloudFront. Your Auto  Scaling group is set to trigger a scale up event at 60% CPU utilization, you use an Amazon RDS extra large  DB instance with  10.000 Provisioned IOPS its CPU utilization is around 80%. While freeable memory is in the 2 GB range.  Web analytics reports show that the average load time of your web pages is around 1 5 to 2 seconds, but  your SEO consultant wants to bring down the average load time to under  0.5 seconds.  How would you improve page load times for your users? (Choose 3 answers)  ",
      "options": [
        {
          "name": "Lower the scale up trigger of your Auto Scaling group to 30% so it scales more aggressively.  ",
          "isAnswer": true,
          "questionId": 54,
          "id": 10540
        },
        {
          "name": "Add an Amazon ElastiCache caching layer to your application for storing sessions and frequent DB queries  ",
          "isAnswer": true,
          "questionId": 54,
          "id": 10541
        },
        {
          "name": "Configure Amazon CloudFront dynamic content support to enable caching of re-usable content from your  site  ",
          "isAnswer": false,
          "questionId": 54,
          "id": 10542
        },
        {
          "name": "Switch Amazon RDS database to the high memory extra large Instance type  ",
          "isAnswer": true,
          "questionId": 54,
          "id": 10543
        },
        {
          "name": "Set up a second installation in another region, and use the Amazon Route 53 latency- based routing  feature to select the right region.",
          "isAnswer": false,
          "questionId": 54,
          "id": 10544
        }
      ],
      "id": 54,
      "questionTypeId": 2,
      "quizId": "40"
    },
    {
      "name": "An International company has deployed a multi-tier web application that relies on DynamoDB in a single  region For regulatory reasons they need disaster recovery capability In a separate region with a Recovery  Time Objective of 2 hours and a Recovery Point Objective of 24 hours They should synchronize their data on  a regular basis and be able to provision me web application rapidly using CloudFormation.  The objective is to minimize changes to the existing web application, control the throughput of DynamoDB  used for the synchronization of data and synchronize only the modified elements.  Which design would you choose to meet these requirements? ",
      "options": [
        {
          "name": "Last updated\\' attribute in your DynamoDB table that would represent the timestamp of thelast updatea and use it as a filter.",
          "isAnswer": false,
          "questionId": 55,
          "id": 10550
        },
        {
          "name": "Use EMR and write a custom script to retrieve data from DynamoDB in the current region using a SCAN  operation and push it to QynamoDB in the second region.  ",
          "isAnswer": false,
          "questionId": 55,
          "id": 10551
        },
        {
          "name": "Use AWS data Pipeline to schedule an export of the DynamoDB table to S3 in the current region once a  day then schedule another task immediately after it that will import data from S3 to DynamoDB in the other  region.  ",
          "isAnswer": true,
          "questionId": 55,
          "id": 10552
        },
        {
          "name": "Send also each Ante into an SQS queue in me second region; use an auto-scaiing group behind the SQS  queue to replay the write in the second region.",
          "isAnswer": false,
          "questionId": 55,
          "id": 10553
        }
      ],
      "id": 55,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You\\'re running an application on-premises due to its dependency on non-x86 hardware and want to use  AWS for data backup. Your backup application is only able to write to POSIX-compatible block-based  storage. You have 140TB of data and would like to mount it as a single folder on your file server Users must  be able to access portions of this data while the backups are taking place. What backup solution would be  most appropriate for this use case?  ",
      "options": [
        {
          "name": "Use Storage Gateway and configure it to use Gateway Cached volumes.  ",
          "isAnswer": false,
          "questionId": 56,
          "id": 10560
        },
        {
          "name": "Configure your backup software to use S3 as the target for your data backups.  ",
          "isAnswer": false,
          "questionId": 56,
          "id": 10561
        },
        {
          "name": "Configure your backup software to use Glacier as the target for your data backups.  ",
          "isAnswer": true,
          "questionId": 56,
          "id": 10562
        },
        {
          "name": "Use Storage Gateway and configure it to use Gateway Stored volumes.",
          "isAnswer": false,
          "questionId": 56,
          "id": 10563
        }
      ],
      "id": 56,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Your system recently experienced down time during the troubleshooting process. You found that a new  administrator mistakenly terminated several production EC2 instances.  Which of the following strategies will help prevent a similar situation in the future? The administrator still must  be able to:  - launch, start stop, and terminate development resources.  ",
      "options": [
        {
          "name": "Create an IAM user, which is not allowed to terminate instances by leveraging production EC2 termination  protection.  ",
          "isAnswer": false,
          "questionId": 57,
          "id": 10570
        },
        {
          "name": "Leverage resource based tagging along with an IAM user, which can prevent specific users from  terminating production EC2 resources.  ",
          "isAnswer": false,
          "questionId": 57,
          "id": 10571
        },
        {
          "name": "Leverage EC2 termination protection and multi-factor authentication, which together require users to  authenticate before terminating EC2 instances ",
          "isAnswer": false,
          "questionId": 57,
          "id": 10572
        },
        {
          "name": "Create an IAM user and apply an IAM role which prevents users from terminating production EC2  instances.",
          "isAnswer": true,
          "questionId": 57,
          "id": 10573
        }
      ],
      "id": 57,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "An AWS customer is deploying an application mat is composed of an AutoScaling group of EC2  Instances.  The customers security policy requires that every outbound connection from these instances to any other  service within the customers  Virtual Private Cloud must be authenticated using a unique x 509 certificate that contains the specific instanceid.  In addition an x 509 certificates must Designed by the customer\\'s Key management service in order to be  trusted for authentication.  Which of the following configurations will support these requirements?  ",
      "options": [
        {
          "name": "Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and  configure me Auto Scaling group to launch instances with this role Have the instances bootstrap get the  certificate from Amazon S3 upon first boot.  ",
          "isAnswer": true,
          "questionId": 58,
          "id": 10580
        },
        {
          "name": "Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group Have the  launched instances generate a certificate signature request with the instance\\'s assigned instance-id to the  Key management service for signature.  ",
          "isAnswer": false,
          "questionId": 58,
          "id": 10581
        },
        {
          "name": "Configure the Auto Scaling group to send an SNS notification of the launch of a new instance to the trusted  key management service. Have the Key management service generate a signed certificate and send it  directly to the newly launched instance.  ",
          "isAnswer": false,
          "questionId": 58,
          "id": 10582
        },
        {
          "name": "Configure the launched instances to generate a new certificate upon first boot Have the Key management  service poll the AutoScaling group for associated instances and send new instances a certificate signature  (hat contains the specific instance-id.",
          "isAnswer": false,
          "questionId": 58,
          "id": 10583
        }
      ],
      "id": 58,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You are implementing AWS Direct Connect. You intend to use AWS public service end points such as  Amazon S3, across the AWS Direct Connect link. You want other Internet traffic to use your existing link to an  Internet Service Provider.  What is the correct way to configure AWS Direct connect for access to services such as Amazon S3?  ",
      "options": [
        {
          "name": "Configure a public Interface on your AWS Direct Connect link Configure a static route via your AWS Direct  Connect link that points to Amazon S3 Advertise a default route to AWS using BGP.  ",
          "isAnswer": false,
          "questionId": 59,
          "id": 10590
        },
        {
          "name": "Create a private interface on your AWS Direct Connect link. Configure a static route via your AWS Direct  connect link that points to Amazon S3 Configure specific routes to your network in your VP",
          "isAnswer": false,
          "questionId": 59,
          "id": 10591
        },
        {
          "name": "Create a public interface on your AWS Direct Connect link Redistribute BGP routes into your existing  routing infrastructure advertise specific routes for your network to AWS.  ",
          "isAnswer": true,
          "questionId": 59,
          "id": 10592
        },
        {
          "name": "Create a private interface on your AWS Direct connect link. Redistribute BGP routes into your existing  routing infrastructure and advertise a default route to AWS.",
          "isAnswer": false,
          "questionId": 59,
          "id": 10593
        }
      ],
      "id": 59,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Your application is using an ELB in front of an Auto Scaling group of web/application servers deployed  across two AZs and a Multi-AZ RDS Instance for data persistence.  The database CPU is often above 80% usage and 90% of I/O operations on the database are reads. To  improve performance you recently added a single-node Memcached ElastiCache Cluster to cache frequent  DB query results. In the next weeks the overall workload is expected to grow by 30%.  Do you need to change anything in the architecture to maintain the high availability or the application with the  anticipated additional load\\'* Why?  ",
      "options": [
        {
          "name": "Yes. you should deploy two Memcached ElastiCache Clusters in different AZs because the ROS Instance  will not Be able to handle the load It me cache node fails.  ",
          "isAnswer": false,
          "questionId": 60,
          "id": 10600
        },
        {
          "name": "No. if the cache node fails the automated ElastiCache node recovery feature will prevent any availability  impact.  ",
          "isAnswer": true,
          "questionId": 60,
          "id": 10601
        },
        {
          "name": "Yes you should deploy the Memcached ElastiCache Cluster with two nodes in the same AZ as the RDS  DB master instance to handle the load if one cache node fails.  ",
          "isAnswer": false,
          "questionId": 60,
          "id": 10602
        },
        {
          "name": "No if the cache node fails you can always get the same data from the DB without having any availability  impact.",
          "isAnswer": false,
          "questionId": 60,
          "id": 10603
        }
      ],
      "id": 60,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Your company currently has a 2-tier web application running in an on-premises data center. You have  experienced several infrastructure failures in the past two months resulting in significant financial losses. Your  CIO is strongly agreeing to move the application to AWS. While working on achieving buy-in from the other  company executives, he asks you to develop a disaster recovery plan to help improve Business continuity in  the short term. He specifies a target Recovery Time Objective (RTO) of 4 hours and a Recovery Point  Objective (RPO) of 1 hour or less. He also asks you to implement the solution within 2 weeks. Your database  is 200GB in size and you have a 20Mbps Internet connection. How would you do this while minimizing costs?  ",
      "options": [
        {
          "name": "Create an EBS backed private AMI which includes a fresh install or your application. Setup a script in your  data center to backup the local database every 1 hour and to encrypt and copy the resulting file to an S3  bucket using multi-part upload.  ",
          "isAnswer": true,
          "questionId": 61,
          "id": 10610
        },
        {
          "name": "Install your application on a compute-optimized EC2 instance capable of supporting the application\\'s  average load synchronously replicate transactions from your on-premises database to a database instance in  AWS across a secure Direct Connect connection. ",
          "isAnswer": false,
          "questionId": 61,
          "id": 10611
        },
        {
          "name": "Deploy your application on EC2 instances within an Auto Scaling group across multiple availability zones  asynchronously replicate transactions from your on-premises database to a database instance in AWS across  a secure VPN connection.  ",
          "isAnswer": false,
          "questionId": 61,
          "id": 10612
        },
        {
          "name": "Create an EBS backed private AMI that includes a fresh install of your application. Develop a Cloud  Formation template which includes your Mil and the required EC2. Auto- Scaling and ELB resources to  support deploying the application across Multiple-Ability Zones. Asynchronously replicate transactions from  your on-premises database to a database instance in AWS across a secure VPN connection.",
          "isAnswer": false,
          "questionId": 61,
          "id": 10613
        }
      ],
      "id": 61,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Which of the following options would you consider for configuring the web server infrastructure? (Choose  2 answers)  ",
      "options": [
        {
          "name": "Configure ELB with TCP listeners on TCP/4d3. And place the Web servers behind it.  ",
          "isAnswer": true,
          "questionId": 62,
          "id": 10620
        },
        {
          "name": "Configure your Web servers with EIPS Place the Web servers in a Route53 Record Set and configure  health checks against all Web servers.  ",
          "isAnswer": true,
          "questionId": 62,
          "id": 10621
        },
        {
          "name": "Configure ELB with HTTPS listeners, and place the Web servers behind it.  ",
          "isAnswer": false,
          "questionId": 62,
          "id": 10622
        },
        {
          "name": "Configure your web servers as the origins for a CloudFront distribution. Use custom SSL certificates on  your CloudFront distribution.",
          "isAnswer": false,
          "questionId": 62,
          "id": 10623
        }
      ],
      "id": 62,
      "questionTypeId": 2,
      "quizId": "40"
    },
    {
      "name": "An administrator is using Amazon CloudFormation to deploy a three tier web application that consists of a  web tier and application tier that will utilize Amazon DynamoDB for storage when creating the CloudFormation  template which of the following would allow the application instance access to the DynamoDB tables without  exposing API credentials?  ",
      "options": [
        {
          "name": "Create an Identity and Access Management Role that has the required permissions to read and write from  the required DynamoDB table and associate the Role to the application instances by referencing an instance  profile.  ",
          "isAnswer": false,
          "questionId": 63,
          "id": 10630
        },
        {
          "name": "Use me Parameter section in the Cloud Formation template to nave the user input Access and Secret Keys  from an already created IAM user that has me permissions required to read and write from the required  DynamoDB table.  ",
          "isAnswer": false,
          "questionId": 63,
          "id": 10631
        },
        {
          "name": "Create an Identity and Access Management Role that has the required permissions to read and write from  the required DynamoDB table and reference the Role in the instance profile property of the application  instance.  ",
          "isAnswer": true,
          "questionId": 63,
          "id": 10632
        },
        {
          "name": "Create an identity and Access Management user in the CioudFormation template that has permissions to  read and write from the required DynamoDB table, use the GetAtt function to retrieve the Access and secret  keys and pass them to the application instance through user-data.",
          "isAnswer": false,
          "questionId": 63,
          "id": 10633
        }
      ],
      "id": 63,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "its tight integration with your developer tools and RDS due to its ease of management.  Your QA team lead points out that you need to roll a sanitized set of production data into your environment on  a nightly basis. Similarly, other software teams in your org want access to that same restored data via their  EC2 instances in your VPC .The optimal setup for persistence and security that meets the above  requirements would be the following.  ",
      "options": [
        {
          "name": "Create your RDS instance as part of your Elastic Beanstalk definition and alter its security group to allow  access to it from hosts in your application subnets.  ",
          "isAnswer": true,
          "questionId": 64,
          "id": 10640
        },
        {
          "name": "Create your RDS instance separately and add its IP address to your application\\'s DB connection strings in  your code Alter its security group to allow access to it from hosts within your VPC\\'s IP address block.  ",
          "isAnswer": false,
          "questionId": 64,
          "id": 10641
        },
        {
          "name": "Create your RDS instance separately and pass its DNS name to your app\\'s DB connection string as an  environment variable. Create a security group for client machines and add it as a valid source for DB traffic to  the security group of the RDS instance itself.  ",
          "isAnswer": false,
          "questionId": 64,
          "id": 10642
        },
        {
          "name": "Create your RDS instance separately and pass its DNS name to your\\'s DB connection string as an  environment variable Alter its security group to allow access to It from hosts In your application subnets.",
          "isAnswer": false,
          "questionId": 64,
          "id": 10643
        }
      ],
      "id": 64,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Your company hosts a social media site supporting users in multiple countries. You have been asked to  provide a highly available design tor the application that leverages multiple regions tor the most recently  accessed content and latency sensitive portions of the wet) site The most latency sensitive component of the  application involves reading user preferences to support web site personalization and ad selection.  In addition to running your application in multiple regions, which option will support this application’s  requirements?  ",
      "options": [
        {
          "name": "Serve user content from S3. CloudFront and use Route53 latency-based routing between ELBs in each  region Retrieve user preferences from a local DynamoDB table in each region and leverage SQS to capture  changes to user preferences with SOS workers for propagating updates to each table.  ",
          "isAnswer": true,
          "questionId": 65,
          "id": 10650
        },
        {
          "name": "Use the S3 Copy API to copy recently accessed content to multiple regions and serve user content from  S3. CloudFront with dynamic content and an ELB in each region Retrieve user preferences from an  ElasticCache cluster in each region and leverage SNS notifications to propagate user preference changes to  a worker node in each region.  ",
          "isAnswer": false,
          "questionId": 65,
          "id": 10651
        },
        {
          "name": "user content from S3 CloudFront and Route53 latency-based routing Between ELBs In  each region Retrieve user preferences from a DynamoDB table and leverage SQS to capture changes to user  preferences with SOS workers for propagating DynamoDB updates.  ",
          "isAnswer": false,
          "questionId": 65,
          "id": 10652
        },
        {
          "name": "Serve user content from S3. CloudFront with dynamic content, and an ELB in each region Retrieve user  preferences from an ElastiCache cluster in each region and leverage Simple Workflow (SWF) to manage the  propagation of user preferences from a centralized OB to each ElastiCache cluster.",
          "isAnswer": false,
          "questionId": 65,
          "id": 10653
        }
      ],
      "id": 65,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "You are looking to migrate your Development (Dev) and Test environments to AWS. You have decided to  use separate AWS accounts to host each environment. You plan to link each accounts bill to a Master AWS  account using Consolidated Billing. To make sure you Keep within budget you would like to implement a way  for administrators in the Master account to have access to stop, delete and/or terminate resources in both the  Dev and Test accounts. Identify which option will allow you to achieve this goal.  ",
      "options": [
        {
          "name": "Create IAM users in the Master account with full Admin permissions. Create cross- account roles in the  Dev and Test accounts that grant the Master account access to the resources in the account by inheriting  permissions from the Master account.  ",
          "isAnswer": true,
          "questionId": 67,
          "id": 10670
        },
        {
          "name": "Create IAM users and a cross-account role in the Master account that grants full Admin permissions to the  Dev and Test accounts.  ",
          "isAnswer": false,
          "questionId": 67,
          "id": 10671
        },
        {
          "name": "Create IAM users in the Master account Create cross-account roles in the Dev and Test accounts that  have full Admin permissions and grant the Master account access.  ",
          "isAnswer": false,
          "questionId": 67,
          "id": 10672
        },
        {
          "name": "Link the accounts using Consolidated Billing. This will give IAM users in the Master account access to  resources in the Dev and Test accounts",
          "isAnswer": false,
          "questionId": 67,
          "id": 10673
        }
      ],
      "id": 67,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Company B is launching a new game app for mobile devices. Users will log into the game using their  existing social media account to streamline data capture. Company B would like to directly save player data  and scoring information from the mobile app to a DynamoDS table named Score Data When a user saves  their game the progress data will be stored to the Game state S3 bucket. what is the best approach for storing  data to DynamoDB and  ",
      "options": [
        {
          "name": "Use an EC2 Instance that is launched with an EC2 role providing access to the Score Data DynamoDB  table and the GameState S3 bucket that communicates with the mobile app via web services.  ",
          "isAnswer": true,
          "questionId": 68,
          "id": 10680
        },
        {
          "name": "Use temporary security credentials that assume a role providing access to the Score Data DynamoDB  table and the Game State S3 bucket using web identity federation.  ",
          "isAnswer": false,
          "questionId": 68,
          "id": 10681
        },
        {
          "name": "Use Login with Amazon allowing users to sign in with an Amazon account providing the mobile app with  access to the Score Data DynamoDB table and the Game State S3 bucket.  ",
          "isAnswer": false,
          "questionId": 68,
          "id": 10682
        },
        {
          "name": "Use an IAM user with access credentials assigned a role providing access to the Score Data DynamoDB  table and the Game State S3 bucket for distribution with the mobile app.",
          "isAnswer": false,
          "questionId": 68,
          "id": 10683
        }
      ],
      "id": 68,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "A customer has a 10 GB AWS Direct Connect connection to an AWS region where they have a web  application hosted on Amazon Elastic Computer Cloud (EC2). The application has dependencies on an onpremises  mainframe database that uses a BASE (Basic Available. Sort stale Eventual consistency) rather than an ACID (Atomicity. Consistency isolation. Durability) consistency model. The application is exhibiting  undesirable behavior because the database is not able to handle the volume of writes. How can you reduce  the load on your on-premises database resources in the most cost-effective way?  ",
      "options": [
        {
          "name": "Use an Amazon Elastic Map Reduce (EMR) S3DistCp as a synchronization mechanism between the onpremises  database and a Hadoop cluster on AWS.  ",
          "isAnswer": true,
          "questionId": 69,
          "id": 10690
        },
        {
          "name": "Modify the application to write to an Amazon SQS queue and develop a worker process to flush the queue  to the on-premises database.  ",
          "isAnswer": false,
          "questionId": 69,
          "id": 10691
        },
        {
          "name": "Modify the application to use DynamoDB to feed an EMR cluster which uses a map function to write to the  on-premises database.  ",
          "isAnswer": false,
          "questionId": 69,
          "id": 10692
        },
        {
          "name": "Provision an RDS read-replica database on AWS to handle the writes and synchronize the two databases  using Data Pipeline.",
          "isAnswer": false,
          "questionId": 69,
          "id": 10693
        }
      ],
      "id": 69,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "on an auto-scaled layer of EC2 instances and an RDS Multi-AZ database Your IT security  compliance officer has tasked you to develop a reliable and durable logging solution to track changes made to  your EC2.IAM And RDS resources. The solution must ensure the integrity and confidentiality of your log data.  Which of these solutions would you recommend?  ",
      "options": [
        {
          "name": "Create a new CloudTrail trail with one new S3 bucket to store the logs and with the global services option  selected Use IAM roles S3 bucket policies and Multi Factor Authentication (MFA) Delete on the S3 bucket  that stores your logs.  ",
          "isAnswer": true,
          "questionId": 70,
          "id": 10700
        },
        {
          "name": "Create a new cloudTrail with one new S3 bucket to store the logs Configure SNS to send log file delivery  notifications to your management system Use IAM roles and S3 bucket policies on the S3 bucket mat stores  your logs.  ",
          "isAnswer": false,
          "questionId": 70,
          "id": 10701
        },
        {
          "name": "Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the global services  option selected Use S3 ACLs and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your  logs.  ",
          "isAnswer": false,
          "questionId": 70,
          "id": 10702
        },
        {
          "name": "Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS  Management console, one for AWS SDKs and one for command line tools Use IAM roles and S3 bucket  policies on the S3 buckets that store your logs.",
          "isAnswer": false,
          "questionId": 70,
          "id": 10703
        }
      ],
      "id": 70,
      "questionTypeId": 1,
      "quizId": "40"
    },
    {
      "name": "Your fortune 500 company has under taken a TCO analysis evaluating the use of Amazon S3 versus  acquiring more hardware The outcome was that ail employees would be granted access to use Amazon S3  for storage of their personal documents.  Which of the following will you need to consider so you can set up a solution that incorporates single sign-on  from your corporate AD or LDAP directory and restricts access for each user to a designated user folder in a bucket? (Choose 3 Answers)  ",
      "options": [
        {
          "name": "Setting up a federation proxy or identity provider  ",
          "isAnswer": true,
          "questionId": 71,
          "id": 10710
        },
        {
          "name": "Using AWS Security Token Service to generate temporary tokens  ",
          "isAnswer": true,
          "questionId": 71,
          "id": 10711
        },
        {
          "name": "Tagging each folder in the bucket  ",
          "isAnswer": true,
          "questionId": 71,
          "id": 10712
        },
        {
          "name": "Configuring IAM role  ",
          "isAnswer": false,
          "questionId": 71,
          "id": 10713
        },
        {
          "name": "Setting up a matching IAM user for every user in your corporate directory that needs access to a folder in  the bucket",
          "isAnswer": false,
          "questionId": 71,
          "id": 10714
        }
      ],
      "id": 71,
      "quizId": "40",
      "questionTypeId": 2
    },
    {
      "name": "Your company has HQ in Tokyo and branch offices all over the world and is using a logistics software with  a multi-regional deployment on AWS in Japan, Europe and USA. The logistic software has a 3-tierarchitecture and currently uses MySQL 5.6 for data persistence. Each region has deployed its own databaseIn the HQ region you run an hourly batch process reading data from every region to compute cross-regionalreports that are sent by email to all offices this batch process must be completed as fast as possible to quicklyoptimize logistics how do you build the database architecture in order to meet the requirements’?",
      "options": [
        {
          "name": "For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ  region  ",
          "isAnswer": true,
          "questionId": 72,
          "id": 10720
        },
        {
          "name": "For each regional deployment, use MySQL on EC2 with a master in the region and send hourly EBS  snapshots to the HQ region  ",
          "isAnswer": false,
          "questionId": 72,
          "id": 10721
        },
        {
          "name": "For each regional deployment, use RDS MySQL with a master in the region and send hourly RDS  snapshots to the HQ region  ",
          "isAnswer": false,
          "questionId": 72,
          "id": 10722
        },
        {
          "name": "For each regional deployment, use MySQL on EC2 with a master in the region and use S3 to copy data  files hourly to the HQ region  ",
          "isAnswer": false,
          "questionId": 72,
          "id": 10723
        },
        {
          "name": "Use Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network  latency for the batch process",
          "isAnswer": false,
          "questionId": 72,
          "id": 10724
        }
      ],
      "id": 72,
      "quizId": "40",
      "questionTypeId": 1
    },
    {
      "name": "You are responsible for a legacy web application whose server environment is approaching end of life  You would like to migrate this application to AWS as quickly as possible, since the application environment  currently has the following limitations:  <li> The VM\\'s single 10GB VMDK is almost full</li>  <li>  Me virtual network interface still uses the 10Mbps driver, which leaves your 100Mbps WAN connection completely underutilized</li> <li> It is currently running on a highly customized. Windows VM within a VMware environment:  <li>  You do not have me installation media</li>  This is a mission critical application with an RTO (Recovery Time Objective) of 8 hours. RPO (Recovery Point  Objective) of 1 hour. How could you best migrate this application to AWS while meeting your business  continuity requirements?  ",
      "options": [
        {
          "name": "Use the EC2 VM Import Connector for vCenter to import the VM into EC2.  ",
          "isAnswer": true,
          "questionId": 73,
          "id": 10730
        },
        {
          "name": "Use Import/Export to import the VM as an ESS snapshot and attach to EC2.  ",
          "isAnswer": false,
          "questionId": 73,
          "id": 10731
        },
        {
          "name": "Use S3 to create a backup of the VM and restore the data into EC2.  ",
          "isAnswer": false,
          "questionId": 73,
          "id": 10732
        },
        {
          "name": "Use me ec2-bundle-instance API to Import an Image of the VM into EC2",
          "isAnswer": false,
          "questionId": 73,
          "id": 10733
        }
      ],
      "id": 73,
      "quizId": "40",
      "questionTypeId": 1
    },
    {
      "name": "You are developing a new mobile application and are considering storing user preferences in AWS.2w  This would provide a more uniform cross-device experience to users using multiple mobile devices to access  the application. The preference data for each user is estimated to be 50KB in size Additionally 5 million  customers are expected to use the application on a regular basis. The solution needs to be cost-effective,  highly available, scalable and secure, how would you design a solution to meet the above requirements?  ",
      "options": [
        {
          "name": "Setup an RDS MySQL instance in 2 availability zones to store the user preference data. Deploy a public  facing application on a server in front of the database to manage security and access credentials  ",
          "isAnswer": false,
          "questionId": 74,
          "id": 10740
        },
        {
          "name": "Setup a DynamoDB table with an item for each user having the necessary attributes to hold the user  preferences. The mobile application will query the user preferences directly from the DynamoDB table. Utilize  STS. Web Identity Federation, and DynamoDB Fine Grained Access Control to authenticate and authorize  access.  ",
          "isAnswer": true,
          "questionId": 74,
          "id": 10741
        },
        {
          "name": "Setup an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user  preference data .The mobile application will query the user preferences from the read replicas. Leverage the  MySQL user management and access privilege system to manage security and access credentials.  ",
          "isAnswer": false,
          "questionId": 74,
          "id": 10742
        },
        {
          "name": "Store the user preference data in S3 Setup a DynamoDB table with an item for each user and an item  attribute pointing to the user’ S3 object. The mobile application will retrieve the S3 URL from DynamoDB and  then access the S3 object directly utilize STS, Web identity Federation, and S3 ACLs to authenticate and  authorize access.",
          "isAnswer": false,
          "questionId": 74,
          "id": 10743
        }
      ],
      "id": 74,
      "quizId": "40",
      "questionTypeId": 1
    },
    {
      "name": "A newspaper organization has a on-premises application which allows the public to search its back  catalogue and retrieve individual newspaper pages via a website written in Java They have scanned the old  newspapers into JPEGs (approx 17TB) and used Optical Character Recognition (OCR) to populate a  commercial search product. The hosting platform and software are now end of life and the organization wants to migrate Its archive to AWS and  produce a cost efficient architecture and still be designed for availability and durability Which is the most  appropriate?  ",
      "options": [
        {
          "name": "Use S3 with reduced redundancy lo store and serve the scanned files, install the commercial search  application on EC2 Instances and configure with auto-scaling and an Elastic Load Balancer.  ",
          "isAnswer": false,
          "questionId": 75,
          "id": 10750
        },
        {
          "name": "Model the environment using CloudFormation use an EC2 instance running Apache webserver and an  open source search application, stripe multiple standard EBS volumes together to store the JPEGs and  search index.  ",
          "isAnswer": true,
          "questionId": 75,
          "id": 10751
        },
        {
          "name": "Use S3 with standard redundancy to store and serve the scanned files, use CloudSearch for query  processing, and use Elastic Beanstalk to host the website across multiple availability zones.  ",
          "isAnswer": false,
          "questionId": 75,
          "id": 10752
        },
        {
          "name": "Use a single-AZ RDS MySQL instance lo store the search index 33d the JPEG images use an EC2  instance to serve the website and translate user queries into SQL.  ",
          "isAnswer": false,
          "questionId": 75,
          "id": 10753
        },
        {
          "name": "Use a CloudFront download distribution to serve the JPEGs to the end users and Install the current  commercial search product, along with a Java Container Tor the website on EC2 instances and use Route53  with DNS round-robin.",
          "isAnswer": false,
          "questionId": 75,
          "id": 10754
        }
      ],
      "id": 75,
      "quizId": "40",
      "questionTypeId": 1
    },
    {
      "name": "Does DynamoDB support in-place atomic updates?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Yes",
          "isAnswer": true
        },
        {
          "name": "No",
          "isAnswer": false
        },
        {
          "name": "It does support in-place non-atomic updates",
          "isAnswer": false
        },
        {
          "name": "It is not defined",
          "isAnswer": false
        }
      ],
      "explanation": "DynamoDB supports in-place atomic updates.",
      "quizId": "41",
      "id": 76
    },
    {
      "name": "Your manager has just given you access to multiple VPN connections that someone else has recently setup between all your company's offices. She needs you to make sure that the communication between theVPNs is secure. Which of the following services would be best for providing a low-cost hub-and-spoke modelfor primary or backup connectMty between these remote offices?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Amazon C|oudFront",
          "isAnswer": false
        },
        {
          "name": "AWS Direct Connect",
          "isAnswer": false
        },
        {
          "name": "AWS C|oudHSM",
          "isAnswer": false
        },
        {
          "name": "AWS VPN CIoudHub",
          "isAnswer": true
        }
      ],
      "explanation": "If you have multiple VPN connections, you can provide secure communication between sites using theAWS VPN CIoudHub. The VPN CIoudHub operates on a simple hub-and-spoke model that you can use withor without a VPC. This design is suitable for customers with multiple branch offices and existing Internetconnections who would like to implement a convenient, potentially low-cost hub-and-spoke model for primaryor backup connectMty between these remote offices.",
      "quizId": "41",
      "id": 77
    },
    {
      "name": "Amazon EC2 provides a . It is an HTTP or HTTPS request that uses the HTTP verbs GET or POST.",
      "questionTypeId": 1,
      "options": [
        {
          "name": "web database",
          "isAnswer": false
        },
        {
          "name": ".net framework",
          "isAnswer": false
        },
        {
          "name": "Query API",
          "isAnswer": true
        },
        {
          "name": "C library",
          "isAnswer": false
        }
      ],
      "explanation": "Amazon EC2 provides a Query API. These requests are HTTP or HTTPS requests that use the HTTP verbsGET or POST and a Query parameter named Action.",
      "quizId": "41",
      "id": 78
    },
    {
      "name": "In Amazon AWS, which of the following statements is true of key pairs?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Key pairs are used only for Amazon SDKs.",
          "isAnswer": false
        },
        {
          "name": "Key pairs are used only for Amazon EC2 and Amazon CIoudFront.",
          "isAnswer": true
        },
        {
          "name": "Key pairs are used only for Elastic Load Balancing and AWS IAM.",
          "isAnswer": false
        },
        {
          "name": "Key pairs are used for all Amazon services.",
          "isAnswer": false
        }
      ],
      "explanation": "Key pairs consist of a public and private key, where you use the private key to create a digital signature, andthen AWS uses the corresponding public key to validate the signature. Key pairs are used only for AmazonEC2 and Amazon CIoudFront.",
      "quizId": "41",
      "id": 79
    },
    {
      "name": "Does Amazon DynamoDB support both increment and decrement atomic operations?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Only increment, since decrement are inherently impossible with DynamoDB's data model.",
          "isAnswer": false
        },
        {
          "name": "No, neither increment nor decrement operations.",
          "isAnswer": false
        },
        {
          "name": "Yes, both increment and decrement operations.",
          "isAnswer": true
        },
        {
          "name": "Only decrement, since increment are inherently impossible with DynamoDB's data model.",
          "isAnswer": false
        }
      ],
      "explanation": "Amazon DynamoDB supports increment and decrement atomic operations.",
      "quizId": "41",
      "id": 80
    },
    {
      "name": "An organization has three separate AWS accounts, one each for development, testing, and production. Theorganization wants the testing team to have access to certain AWS resources in the production account. Howcan the organization achieve this?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "It is not possible to access resources of one account with another account.",
          "isAnswer": false
        },
        {
          "name": "Create the IAM roles with cross account access.",
          "isAnswer": true
        },
        {
          "name": "Create the IAM user in a test account, and allow it access to the production environment with the IAMpolicy.",
          "isAnswer": false
        },
        {
          "name": "Create the IAM users with cross account access.",
          "isAnswer": false
        }
      ],
      "explanation": "An organization has multiple AWS accounts to isolate a development environment from a testing orproduction environment. At times the users from one account need to access resources in the other account,such as promoting an update from the development environment to the production environment. In this casethe IAM role with cross account access will provide a solution. Cross account access lets one account shareaccess to their resources with users in the other AWS accounts.",
      "quizId": "41",
      "id": 81
    },
    {
      "name": "You need to import several hundred megabytes of data from a local Oracle database to an Amazon RDSDB instance. What does AWS recommend you use to accomplish this?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Oracle export/import utilities",
          "isAnswer": false
        },
        {
          "name": "Oracle SQL Developer",
          "isAnswer": false
        },
        {
          "name": "Oracle Data Pump",
          "isAnswer": true
        },
        {
          "name": "DBMS_FILE_TRANSFER",
          "isAnswer": false
        }
      ],
      "explanation": "How you import data into an Amazon RDS DB instance depends on the amount of data you have and thenumber and variety of database objects in your database.<br>For example, you can use Oracle SQL Developer to import a simple, 20 MB database; you want to useOracle Data Pump to import complex databases or databases that are several hundred megabytes or severalterabytes in size.",
      "quizId": "41",
      "id": 82
    },
    {
      "name": "A user has created an EBS volume with 1000 IOPS. What is the average IOPS that the user will get formost of the year as per EC2 SLA if the instance is attached to the EBS optimized instance?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "950",
          "isAnswer": false
        },
        {
          "name": "990",
          "isAnswer": false
        },
        {
          "name": "1000",
          "isAnswer": false
        },
        {
          "name": "900",
          "isAnswer": true
        }
      ],
      "explanation": "As per AWS SLA if the instance is attached to an EBS-Optimized instance, then the Provisioned IOPSvolumes are designed to deliver within 10% of the provisioned IOPS performance 99.9% of the time in a givenyear. Thus, if the user has created a volume of 1000 IOPS, the user will get a minimum 900 IOPS 99.9% timeof the year.",
      "quizId": "41",
      "id": 83
    },
    {
      "name": "You need to migrate a large amount of data into the cloud that you have stored on a hard disk and youdecide that the best way to accomplish this is with AWS Import/Export and you mail the hard disk to AWS.Which of the following statements is incorrect in regards to AWS Import/Export?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "It can export from Amazon S3",
          "isAnswer": false
        },
        {
          "name": "It can Import to Amazon Glacier",
          "isAnswer": false
        },
        {
          "name": "It can export from Amazon Glacier.",
          "isAnswer": true
        },
        {
          "name": "It can Import to Amazon EBS",
          "isAnswer": false
        }
      ],
      "explanation": "AWS Import/Export supports: Import to Amazon S3Export from Amazon S3 Import to Amazon EBS Import to Amazon Glacier",
      "quizId": "41",
      "id": 84
    },
    {
      "name": "You are in the process of creating a Route 53 DNS failover to direct traffic to two EC2 zones. Obviously, ifone fails, you would like Route 53 to direct traffic to the other region. Each region has an ELB with someinstances being distributed. What is the best way for you to configure the Route 53 health check?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Route 53 doesn't support ELB with an internal health check.You need to create your own Route 53 healthcheck of the ELB",
          "isAnswer": false
        },
        {
          "name": "Route 53 natively supports ELB with an internal health check. Turn \"Eva|uate target health\" off and\"Associate with Health Check\" on and R53 will use the ELB's internal health check.",
          "isAnswer": false
        },
        {
          "name": "Route 53 doesn't support ELB with an internal health check. You need to associate your resource recordset for the ELB with your own health check",
          "isAnswer": false
        },
        {
          "name": "Route 53 natively supports ELB with an internal health check. Turn \"Eva|uate target health\" on and\"Associate with Health Check\" off and R53 will use the ELB's internal health check.",
          "isAnswer": true
        }
      ],
      "explanation": "With DNS Failover, Amazon Route 53 can help detect an outage of your website and redirect your end usersto alternate locations where your application is operating properly. When you enable this feature, Route 53uses health checks-regularly making Internet requests to your appIication’s endpoints from multiple locations around the world-to determine whether each endpoint of your application is up or down.<br>To enable DNS Failover for an ELB endpoint, create an Alias record pointing to the ELB and set the\"EvaIuate Target HeaIth\" parameter to true. Route 53 creates and manages the health checks for your ELBautomatically. You do not need to create your own Route 53 health check of the ELB. You also do not need toassociate your resource record set for the ELB with your own health check, because Route 53 automaticallyassociates it with the health checks that Route 53 manages on your behalf. The ELB health check will alsoinherit the health of your backend instances behind that ELB.",
      "quizId": "41",
      "id": 85
    },
    {
      "name": "A user wants to use an EBS-backed Amazon EC2 instance for a temporary job. Based on the input data,the job is most likely to finish within a week. Which of the following steps should be followed to terminate theinstance automatically once the job is finished?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Configure the EC2 instance with a stop instance to terminate it.",
          "isAnswer": false
        },
        {
          "name": "Configure the EC2 instance with ELB to terminate the instance when it remains idle.",
          "isAnswer": false
        },
        {
          "name": "Configure the CIoudWatch alarm on the instance that should perform the termination action once theinstance is idle.",
          "isAnswer": true
        },
        {
          "name": "Configure the Auto Scaling schedule actMty that terminates the instance after 7 days.",
          "isAnswer": false
        }
      ],
      "explanation": "Auto Scaling can start and stop the instance at a pre-defined time. Here, the total running time is unknown.Thus, the user has to use the CIoudWatch alarm, which monitors the CPU utilization. The user can create analarm that is triggered when the average CPU utilization percentage has been lower than 10 percentfor 24 hours, signaling that it is idle and no longer in use. When the utilization is below the threshold limit, itwill terminate the instance as a part of the instance action.",
      "quizId": "41",
      "id": 86
    },
    {
      "name": "Which of the following is true of Amazon EC2 security group?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "You can modify the outbound rules for EC2-Classic.",
          "isAnswer": false
        },
        {
          "name": "You can modify the rules for a security group only if the security group controls the traffic for just oneinstance.",
          "isAnswer": false
        },
        {
          "name": "You can modify the rules for a security group only when a new instance is created.",
          "isAnswer": false
        },
        {
          "name": "You can modify the rules for a security group at any time.",
          "isAnswer": true
        }
      ],
      "explanation": "A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launchan instance, you associate one or more security groups with the instance. You add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at anytime; the new rules are automatically applied to all instances that are associated with the security group.",
      "quizId": "41",
      "id": 87
    },
    {
      "name": "An Elastic IP address (EIP) is a static IP address designed for dynamic cloud computing. With an EIP, youcan mask the failure of an instance or software by rapidly remapping the address to another instance in youraccount. Your EIP is associated with your AWS account, not a particular EC2 instance, and it remainsassociated with your account until you choose to explicitly release it. By default how many EIPs is each AWSaccount limited to on a per region basis?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "1",
          "isAnswer": false
        },
        {
          "name": "5",
          "isAnswer": true
        },
        {
          "name": "Unlimited",
          "isAnswer": false
        },
        {
          "name": "10",
          "isAnswer": false
        }
      ],
      "explanation": "By default, all AWS accounts are limited to 5 Elastic IP addresses per region for each AWS account, becausepublic (IPv4) Internet addresses are a scarce public resource. AWS strongly encourages you to use an EIPprimarily for load balancing use cases, and use DNS hostnames for all other inter-node communication.<br>If you feel your architecture warrants additional EIPs, you would need to complete the Amazon EC2 Elastic IPAddress Request Form and give reasons as to your need for additional addresses.",
      "quizId": "41",
      "id": 88
    },
    {
      "name": "In Amazon EC2, partial instance-hours are billed .",
      "questionTypeId": 1,
      "options": [
        {
          "name": "per second used in the hour",
          "isAnswer": false
        },
        {
          "name": "per minute used",
          "isAnswer": false
        },
        {
          "name": "by combining partial segments into full hours",
          "isAnswer": false
        },
        {
          "name": "as full hours",
          "isAnswer": true
        }
      ],
      "explanation": "Partial instance-hours are billed to the next hour.",
      "quizId": "41",
      "id": 89
    },
    {
      "name": "In EC2, what happens to the data in an instance store if an instance reboots (either intentionally orunintentionally)?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Data is deleted from the instance store for security reasons.",
          "isAnswer": false
        },
        {
          "name": "Data persists in the instance store.",
          "isAnswer": true
        },
        {
          "name": "Data is partially present in the instance store.",
          "isAnswer": false
        },
        {
          "name": "Data in the instance store will be lost.",
          "isAnswer": false
        }
      ],
      "explanation": "The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots(intentionally or unintentionally), data in the instance store persists. However, data on instance store volumesis lost under the following circumstances.Failure of an underlying driveStopping an Amazon EBS-backed instance Terminating an instance",
      "quizId": "41",
      "id": 90
    },
    {
      "name": "You are setting up a VPC and you need to set up a public subnet within that VPC. Which following requirement must be met for this subnet to be considered a public subnet?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Subnet's traffic is not routed to an internet gateway but has its traffic routed to a virtual private gateway.",
          "isAnswer": false
        },
        {
          "name": "Subnet's traffic is routed to an internet gateway.",
          "isAnswer": true
        },
        {
          "name": "Subnet's traffic is not routed to an internet gateway.",
          "isAnswer": false
        },
        {
          "name": "None of these answers can be considered a public subnet.",
          "isAnswer": false
        }
      ],
      "explanation": "A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS cloud. You can launch your AWS resources, such as Amazon EC2 instances, into your VPC. You can configure your VPC: you can select its IP address range, create subnets, and configure route tables, network gateways, and security settings.<br>A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a subnet that you select. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won't be connected to the Internet.<br>If a subnet's traffic is routed to an internet gateway, the subnet is known as a public subnet.<br>If a subnet doesn't have a route to the internet gateway, the subnet is known as a private subnet.<br>If a subnet doesn't have a route to the internet gateway, but has its traffic routed to a virtual private gateway, the subnet is known as a VPN-only subnet.",
      "quizId": "41",
      "id": 91
    },
    {
      "name": "Can you specify the security group that you created for a VPC when you launch an instance in EC2-Classic?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "No, you can specify the security group created for EC2-Classic when you launch a VPC instance.",
          "isAnswer": false
        },
        {
          "name": "No",
          "isAnswer": true
        },
        {
          "name": "Yes",
          "isAnswer": false
        },
        {
          "name": "No, you can specify the security group created for EC2-Classic to a non-VPC based instance only.",
          "isAnswer": false
        }
      ],
      "explanation": "If you're using EC2-Classic, you must use security groups created specifically for EC2-Classic. When youlaunch an instance in EC2-Classic, you must specify a security group in the same region as the instance. Youcan't specify a security group that you created for a VPC when you launch an instance inEC2-Classic.",
      "quizId": "41",
      "id": 92
    },
    {
      "name": "While using the EC2 GET requests as URLs, the is the URL that serves as the entry point for the webservice.",
      "questionTypeId": 1,
      "options": [
        {
          "name": "token",
          "isAnswer": false
        },
        {
          "name": "endpoint",
          "isAnswer": true
        },
        {
          "name": "action",
          "isAnswer": false
        },
        {
          "name": "None of these",
          "isAnswer": false
        }
      ],
      "explanation": "The endpoint is the URL that serves as the entry point for the web service.",
      "quizId": "41",
      "id": 93
    },
    {
      "name": "You have been asked to build a database warehouse using Amazon Redshift. You know a little about it,including that it is a SQL data warehouse solution, and uses industry standard ODBC and JDBC connectionsand PostgreSQL drivers. However you are not sure about what sort of storage it uses for database tables.What sort of storage does Amazon Redshift use for database tables?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "InnoDB Tables",
          "isAnswer": false
        },
        {
          "name": "NDB data storage",
          "isAnswer": false
        },
        {
          "name": "Columnar data storage",
          "isAnswer": true
        },
        {
          "name": "NDB CLUSTER Storage",
          "isAnswer": false
        }
      ],
      "explanation": "Amazon Redshift achieves efficient storage and optimum query performance through a combination ofmassively parallel processing, columnar data storage, and very efficient, targeted data compression encodingschemes.<br>Columnar storage for database tables is an important factor in optimizing analytic query performance becauseit drastically reduces the overall disk I/O requirements and reduces the amount of data you need to load fromdisk.",
      "quizId": "41",
      "id": 94
    },
    {
      "name": "You are checking the workload on some of your General Purpose (SSD) and Provisioned IOPS (SSD)volumes and it seems that the I/O latency is higher than you require. You should probably check the to makesure that your application is not trying to drive more IOPS than you haveprovisioned.",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Amount of IOPS that are available",
          "isAnswer": false
        },
        {
          "name": "Acknowledgement from the storage subsystem",
          "isAnswer": false
        },
        {
          "name": "Average queue length",
          "isAnswer": true
        },
        {
          "name": "Time it takes for the I/O operation to complete",
          "isAnswer": false
        }
      ],
      "explanation": "In EBS workload demand plays an important role in getting the most out of your General Purpose (SSD) and Provisioned IOPS (SSD) volumes. In order for your volumes to deliver the amount of IOPS that are available, they need to have enough I/O requests sent to them. There is a relationship between the demand on the volumes, the amount of IOPS that are available to them, and the latency of the request (the amount of time it takes for the I/O operation to complete).<br>Latency is the true end-to-end client time of an I/O operation; in other words, when the client sends a IO, how long does it take to get an acknowledgement from the storage subsystem that the IO read or write is complete.<br>If your I/O latency is higher than you require, check your average queue length to make sure that your application is not trying to drive more IOPS than you have provisioned. You can maintain high IOPS while keeping latency down by maintaining a low average queue length (which is achieved by provisioning more IOPS for your volume).",
      "quizId": "41",
      "id": 95
    },
    {
      "name": "Which of the below mentioned options is not available when an instance is launched by Auto Scaling withEC2 Classic?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Public IP ",
          "isAnswer": false
        },
        {
          "name": "Elastic IP",
          "isAnswer": true
        },
        {
          "name": "Private DNS",
          "isAnswer": false
        },
        {
          "name": "Private IP",
          "isAnswer": false
        }
      ],
      "explanation": "Auto Scaling supports both EC2 classic and EC2-VPC. When an instance is launched as a part of EC2classic, it will have the public IP and DNS as well as the private IP and DNS.",
      "quizId": "41",
      "id": 96
    },
    {
      "name": "You have been given a scope to deploy some AWS infrastructure for a large organisation. Therequirements are that you will have a lot of EC2 instances but may need to add more when the averageutilization of your Amazon EC2 fileet is high and conversely remove them when CPU utilization is low. WhichAWS services would be best to use to accomplish this?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Auto Scaling, Amazon CIoudWatch and AWS Elastic Beanstalk",
          "isAnswer": false
        },
        {
          "name": "Auto Scaling, Amazon CIoudWatch and Elastic Load Balancing.",
          "isAnswer": true
        },
        {
          "name": "Amazon CIoudFront, Amazon CIoudWatch and Elastic Load Balancing.",
          "isAnswer": false
        },
        {
          "name": "AWS Elastic Beanstalk , Amazon CIoudWatch and Elastic Load Balancing.",
          "isAnswer": false
        }
      ],
      "explanation": "Auto Scaling enables you to follow the demand curve for your applications closely, reducing the need tomanually provision Amazon EC2 capacity in advance. For example, you can set a condition to add newAmazon EC2 instances in increments to the Auto Scaling group when the average utilization of your AmazonEC2 fileet is high; and similarly, you can set a condition to remove instances in the same increments whenCPU utilization is low. If you have predictable load changes, you can set a schedule through Auto Scaling toplan your scaling actMties. You can use Amazon CIoudWatch to send alarms to trigger scaling actMties andElastic Load Balancing to help distribute traffic to your instances within Auto Scaling groups. Auto Scalingenables you to run your Amazon EC2 fileet at optimal utilization.",
      "quizId": "41",
      "id": 97
    },
    {
      "name": "You are building infrastructure for a data warehousing solution and an extra request has come throughthat there will be a lot of business reporting queries running all the time and you are not sure if your currentDB instance will be able to handle it. What would be the best solution for this?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "DB Parameter Groups",
          "isAnswer": false
        },
        {
          "name": "Read Replicas",
          "isAnswer": true
        },
        {
          "name": "Multi-AZ DB Instance deployment",
          "isAnswer": false
        },
        {
          "name": "Database Snapshots",
          "isAnswer": false
        }
      ],
      "explanation": "Read Replicas make it easy to take advantage of MySQL’s built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. There are a variety of scenarios where deploying one or more Read Replicas for a given source DB Instance may make sense. Common reasons for deploying a Read Replica include:<br>Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas.<br>Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read RepIica(s). For this use case, keep in mind that the data on the Read Replica may be \"staIe\" since the source DB Instance is unavailable.<br>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance.",
      "quizId": "41",
      "id": 98
    },
    {
      "name": "In DynamoDB, could you use IAM to grant access to Amazon DynamoDB resources and API actions?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "In DynamoDB there is no need to grant access",
          "isAnswer": false
        },
        {
          "name": "Depended to the type of access",
          "isAnswer": false
        },
        {
          "name": "No",
          "isAnswer": false
        },
        {
          "name": "Yes",
          "isAnswer": true
        }
      ],
      "explanation": "Amazon DynamoDB integrates with AWS Identity and Access Management (IAM). You can use AWS IAM togrant access to Amazon DynamoDB resources and API actions. To do this, you first write an AWS IAM policy,which is a document that explicitly lists the permissions you want to grant. You then attach that policy to anAWS IAM user or role.",
      "quizId": "41",
      "id": 99
    },
    {
      "name": "Much of your company's data does not need to be accessed often, and can take several hours forretrieval time, so it's stored on Amazon Glacier. However someone within your organization has expressedconcerns that his data is more sensitive than the other data, and is wondering whether the highlevel of encryption that he knows is on S3 is also used on the much cheaper Glacier service. Which of thefollowing statements would be most applicable in regards to this concern?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "There is no encryption on Amazon Glacier, that's why it is cheaper.",
          "isAnswer": false
        },
        {
          "name": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than AmazonS3 but you can change it to AES-256 if you are willing to pay more.",
          "isAnswer": false
        },
        {
          "name": "Amazon Glacier automatically encrypts the data using AES-256, the same as Amazon S3.",
          "isAnswer": true
        },
        {
          "name": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than AmazonS3.",
          "isAnswer": false
        }
      ],
      "explanation": "Like Amazon S3, the Amazon Glacier service provides low-cost, secure, and durable storage. But where S3is designed for rapid retrieval, Glacier is meant to be used as an archival service for data that is not accessedoften, and for which retrieval times of several hours are suitable.<br>Amazon Glacier automatically encrypts the data using AES-256 and stores it durably in an immutable form.Amazon Glacier is designed to provide average annual durability of 99.999999999% for an archive. It storeseach archive in multiple facilities and multiple devices. Unlike traditional systems which can require laboriousdata verification and manual repair, Glacier performs regular, systematic data integrity checks, and is built tobe automatically self-healing.",
      "quizId": "41",
      "id": 100
    },
    {
      "name": "Your EBS volumes do not seem to be performing as expected and your team leader has requested youlook into improving their performance. Which of the following is not a true statement relating to theperformance of your EBS volumes?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Frequent snapshots provide a higher level of data durability and they will not degrade the performance ofyour application while the snapshot is in progress.",
          "isAnswer": true
        },
        {
          "name": "General Purpose (SSD) and Provisioned IOPS (SSD) volumes have a throughput limit of 128 MB/s pervolume.",
          "isAnswer": false
        },
        {
          "name": "There is a relationship between the maximum performance of your EBS volumes, the amount of I/O youare drMng to them, and the amount of time it takes for each transaction to complete.",
          "isAnswer": false
        },
        {
          "name": "There is a 5 to 50 percent reduction in IOPS when you first access each block of data on a newly createdor restored EBS volume",
          "isAnswer": false
        }
      ],
      "explanation": "Several factors can affect the performance of Amazon EBS volumes, such as instance configuration, I/Ocharacteristics, workload demand, and storage configuration.<br>Frequent snapshots provide a higher level of data durability, but they may slightly degrade theperformance of your application while the snapshot is in progress. This trade off becomes critical when youhave data that changes rapidly. Whenever possible, plan for snapshots to occur during off-peak times in orderto minimize workload impact.",
      "quizId": "41",
      "id": 101
    },
    {
      "name": "You've created your first load balancer and have registered your EC2 instances with the load balancer.Elastic Load Balancing routinely performs health checks on all the registered EC2 instances andautomatically distributes all incoming requests to the DNS name of your load balancer across your registered,healthy EC2 instances. By default, the load balancer uses the _ protocol for checking the health of yourinstances.",
      "questionTypeId": 1,
      "options": [
        {
          "name": "HTTPS",
          "isAnswer": false
        },
        {
          "name": "HTTP",
          "isAnswer": true
        },
        {
          "name": "ICMP",
          "isAnswer": false
        },
        {
          "name": "IPv6",
          "isAnswer": false
        }
      ],
      "explanation": "In Elastic Load Balancing a health configuration uses information such as protocol, ping port, ping path (URL),response timeout period, and health check interval to determine the health state of the instances registeredwith the load balancer.<br>Currently, HTTP on port 80 is the default health check.",
      "quizId": "41",
      "id": 102
    },
    {
      "name": "A major finance organisation has engaged your company to set up a large data mining application. UsingAWS you decide the best service for this is Amazon Elastic MapReduce(EMR) which you know uses Hadoop.Which of the following statements best describes Hadoop?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Hadoop is 3rd Party software which can be installed using AMI",
          "isAnswer": false
        },
        {
          "name": "Hadoop is an open source python web framework",
          "isAnswer": false
        },
        {
          "name": "Hadoop is an open source Java software framework",
          "isAnswer": true
        },
        {
          "name": "Hadoop is an open source javascript framework",
          "isAnswer": false
        }
      ],
      "explanation": "Amazon EMR uses Apache Hadoop as its distributed data processing engine.<br>Hadoop is an open source, Java software framework that supports data-intensive distributed applicationsrunning on large clusters of commodity hardware. Hadoop implements a programming model named\"MapReduce,\" where the data is dMded into many small fragments of work, each of which may be executedon any node in the cluster.<br>This framework has been widely used by developers, enterprises and startups and has proven to be a reliablesoftware platform for processing up to petabytes of data on clusters of thousands of commodity machines.",
      "quizId": "41",
      "id": 103
    },
    {
      "name": "In Amazon EC2 Container Service, are other container types supported?",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Yes, EC2 Container Service supports any container service you need.",
          "isAnswer": false
        },
        {
          "name": "Yes, EC2 Container Service also supports Microsoft container service.",
          "isAnswer": false
        },
        {
          "name": "No, Docker is the only container platform supported by EC2 Container Service presently.",
          "isAnswer": true
        },
        {
          "name": "Yes, EC2 Container Service supports Microsoft container service and Openstack.",
          "isAnswer": false
        }
      ],
      "explanation": "In Amazon EC2 Container Service, Docker is the only container platform supported by EC2 Container Servicepresently.",
      "quizId": "41",
      "id": 104
    },
    {
      "name": "is a fast, filexible, fully managed push messaging service.",
      "questionTypeId": 1,
      "options": [
        {
          "name": "Amazon SNS",
          "isAnswer": true
        },
        {
          "name": "Amazon SES",
          "isAnswer": false
        },
        {
          "name": "Amazon SQS",
          "isAnswer": false
        },
        {
          "name": "Amazon FPS",
          "isAnswer": false
        }
      ],
      "explanation": "Amazon Simple Notification Service (Amazon SNS) is a fast, filexible, fully managed push messaging service.Amazon SNS makes it simple and cost-effective to push to mobile devices such as iPhone, iPad, Android,Kindle Fire, and internet connected smart devices, as well as pushing to other distributed services.",
      "quizId": "41",
      "id": 105
    }
  ]
}